{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T15:37:21.300830Z",
     "iopub.status.busy": "2025-03-05T15:37:21.300514Z",
     "iopub.status.idle": "2025-03-05T15:37:46.498968Z",
     "shell.execute_reply": "2025-03-05T15:37:46.498186Z",
     "shell.execute_reply.started": "2025-03-05T15:37:21.300808Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting auto_gptq\n",
      "  Downloading auto_gptq-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from auto_gptq) (1.2.1)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from auto_gptq) (3.3.1)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from auto_gptq) (0.2.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from auto_gptq) (1.26.4)\n",
      "Collecting rouge (from auto_gptq)\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting gekko (from auto_gptq)\n",
      "  Downloading gekko-1.2.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from auto_gptq) (2.5.1+cu121)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from auto_gptq) (0.4.5)\n",
      "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from auto_gptq) (4.47.0)\n",
      "Requirement already satisfied: peft>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from auto_gptq) (0.14.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from auto_gptq) (4.67.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto_gptq) (24.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto_gptq) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto_gptq) (6.0.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto_gptq) (0.29.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->auto_gptq) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->auto_gptq) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->auto_gptq) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->auto_gptq) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->auto_gptq) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->auto_gptq) (2.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto_gptq) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto_gptq) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto_gptq) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto_gptq) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto_gptq) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto_gptq) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->auto_gptq) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto_gptq) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto_gptq) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto_gptq) (0.21.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->auto_gptq) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->auto_gptq) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->auto_gptq) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->auto_gptq) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets->auto_gptq) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->auto_gptq) (3.11.12)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge->auto_gptq) (1.17.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto_gptq) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto_gptq) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto_gptq) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto_gptq) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto_gptq) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto_gptq) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto_gptq) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto_gptq) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto_gptq) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto_gptq) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto_gptq) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto_gptq) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->auto_gptq) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->auto_gptq) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->auto_gptq) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->auto_gptq) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->auto_gptq) (2024.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto_gptq) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto_gptq) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto_gptq) (2025.1)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->auto_gptq) (2024.2.0)\n",
      "Downloading auto_gptq-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.5/23.5 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading gekko-1.2.1-py3-none-any.whl (13.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: rouge, gekko, auto_gptq\n",
      "Successfully installed auto_gptq-0.7.1 gekko-1.2.1 rouge-1.0.1\n",
      "Collecting optimum\n",
      "  Downloading optimum-1.24.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: transformers>=4.29 in /usr/local/lib/python3.10/dist-packages (from optimum) (4.47.0)\n",
      "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.10/dist-packages (from optimum) (2.5.1+cu121)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from optimum) (24.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optimum) (1.26.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from optimum) (0.29.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum) (2024.12.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.8.0->optimum) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->optimum) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->optimum) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->optimum) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11->optimum) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.29->optimum) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.29->optimum) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.29->optimum) (0.4.5)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->optimum) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->optimum) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->optimum) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->optimum) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->optimum) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->optimum) (2.4.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11->optimum) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->optimum) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->optimum) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->optimum) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->optimum) (2024.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.8.0->optimum) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.8.0->optimum) (2025.1.31)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->optimum) (2024.2.0)\n",
      "Downloading optimum-1.24.0-py3-none-any.whl (433 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.6/433.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: optimum\n",
      "Successfully installed optimum-1.24.0\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.4.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.1+cu121)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.29.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3.0.0,>=1.17->accelerate) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\n",
      "Downloading accelerate-1.4.0-py3-none-any.whl (342 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.1/342.1 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m108.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: transformers, accelerate, datasets, bitsandbytes\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.47.0\n",
      "    Uninstalling transformers-4.47.0:\n",
      "      Successfully uninstalled transformers-4.47.0\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.2.1\n",
      "    Uninstalling accelerate-1.2.1:\n",
      "      Successfully uninstalled accelerate-1.2.1\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 3.3.1\n",
      "    Uninstalling datasets-3.3.1:\n",
      "      Successfully uninstalled datasets-3.3.1\n",
      "Successfully installed accelerate-1.4.0 bitsandbytes-0.45.3 datasets-3.3.2 transformers-4.49.0\n"
     ]
    }
   ],
   "source": [
    "!pip install auto_gptq\n",
    "!pip install optimum\n",
    "!pip install -U accelerate bitsandbytes datasets peft transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T15:37:46.500635Z",
     "iopub.status.busy": "2025-03-05T15:37:46.500325Z",
     "iopub.status.idle": "2025-03-05T15:37:50.276516Z",
     "shell.execute_reply": "2025-03-05T15:37:50.275776Z",
     "shell.execute_reply.started": "2025-03-05T15:37:46.500602Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from transformers.utils import is_auto_gptq_available,   is_optimum_available\n",
    "print(is_auto_gptq_available())\n",
    "print(is_optimum_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-03-05T15:37:50.278252Z",
     "iopub.status.busy": "2025-03-05T15:37:50.277891Z",
     "iopub.status.idle": "2025-03-05T15:38:09.890344Z",
     "shell.execute_reply": "2025-03-05T15:38:09.889226Z",
     "shell.execute_reply.started": "2025-03-05T15:37:50.278227Z"
    },
    "executionInfo": {
     "elapsed": 110695,
     "status": "ok",
     "timestamp": 1715498221178,
     "user": {
      "displayName": "Shree Krishna Jamakatel",
      "userId": "14328212804148880429"
     },
     "user_tz": -345
    },
    "id": "RuYiH3p9IeIF",
    "outputId": "f65b78d1-23a7-4f11-a00f-7555ff39237c",
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets==2.15.0\n",
      "  Downloading datasets-2.15.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==2.15.0) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.15.0) (19.0.1)\n",
      "Collecting pyarrow-hotfix (from datasets==2.15.0)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets==2.15.0)\n",
      "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.15.0) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.15.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.15.0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==2.15.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets==2.15.0) (0.70.16)\n",
      "Collecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.15.0)\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.15.0) (3.11.12)\n",
      "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.15.0) (0.29.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.15.0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.15.0) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.15.0) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.15.0) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.15.0) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.15.0) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.15.0) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.15.0) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.15.0) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.15.0) (1.18.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets==2.15.0) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets==2.15.0) (4.12.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets==2.15.0) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets==2.15.0) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets==2.15.0) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets==2.15.0) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets==2.15.0) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets==2.15.0) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.15.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.15.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.15.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.15.0) (2025.1.31)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets==2.15.0)\n",
      "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
      "  Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.15.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.15.0) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.15.0) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.15.0) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets==2.15.0) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets==2.15.0) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets==2.15.0) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->datasets==2.15.0) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->datasets==2.15.0) (2024.2.0)\n",
      "Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Installing collected packages: pyarrow-hotfix, fsspec, dill, multiprocess, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.12.0\n",
      "    Uninstalling fsspec-2024.12.0:\n",
      "      Successfully uninstalled fsspec-2024.12.0\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.8\n",
      "    Uninstalling dill-0.3.8:\n",
      "      Successfully uninstalled dill-0.3.8\n",
      "  Attempting uninstall: multiprocess\n",
      "    Found existing installation: multiprocess 0.70.16\n",
      "    Uninstalling multiprocess-0.70.16:\n",
      "      Successfully uninstalled multiprocess-0.70.16\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 3.3.2\n",
      "    Uninstalling datasets-3.3.2:\n",
      "      Successfully uninstalled datasets-3.3.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2023.10.0 which is incompatible.\n",
      "pathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.7 which is incompatible.\n",
      "pathos 0.3.2 requires multiprocess>=0.70.16, but you have multiprocess 0.70.15 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-2.15.0 dill-0.3.7 fsspec-2023.10.0 multiprocess-0.70.15 pyarrow-hotfix-0.6\n",
      "Collecting trl\n",
      "  Downloading trl-0.15.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: accelerate>=0.34.0 in /usr/local/lib/python3.10/dist-packages (from trl) (1.4.0)\n",
      "Collecting datasets>=2.21.0 (from trl)\n",
      "  Using cached datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl) (13.9.4)\n",
      "Requirement already satisfied: transformers>=4.46.0 in /usr/local/lib/python3.10/dist-packages (from trl) (4.49.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (24.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (2.5.1+cu121)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (0.29.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (0.4.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (3.17.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.21.0->trl) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (3.11.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.0->trl) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.0->trl) (0.21.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (2.19.1)\n",
      "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (4.12.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.18.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (2025.1.31)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=0.34.0->trl) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.21.0->trl) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.21.0->trl) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.21.0->trl) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.21.0->trl) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->accelerate>=0.34.0->trl) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3.0.0,>=1.17->accelerate>=0.34.0->trl) (2024.2.0)\n",
      "Downloading trl-0.15.2-py3-none-any.whl (318 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached datasets-3.3.2-py3-none-any.whl (485 kB)\n",
      "Installing collected packages: datasets, trl\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.15.0\n",
      "    Uninstalling datasets-2.15.0:\n",
      "      Successfully uninstalled datasets-2.15.0\n",
      "Successfully installed datasets-3.3.2 trl-0.15.2\n",
      "Looking in indexes: https://pypi.org/simple, https://huggingface.github.io/autogptq-index/whl/cu117/\n",
      "Requirement already satisfied: auto-gptq in /usr/local/lib/python3.10/dist-packages (0.7.1)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (1.4.0)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (3.3.2)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.2.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (1.26.4)\n",
      "Requirement already satisfied: rouge in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (1.0.1)\n",
      "Requirement already satisfied: gekko in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (1.2.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (2.5.1+cu121)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.4.5)\n",
      "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (4.49.0)\n",
      "Requirement already satisfied: peft>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.14.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (4.67.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto-gptq) (24.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto-gptq) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto-gptq) (6.0.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto-gptq) (0.29.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->auto-gptq) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->auto-gptq) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->auto-gptq) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->auto-gptq) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->auto-gptq) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->auto-gptq) (2.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (2023.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->auto-gptq) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (0.21.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (3.11.12)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge->auto-gptq) (1.17.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->auto-gptq) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->auto-gptq) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->auto-gptq) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->auto-gptq) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->auto-gptq) (2024.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2025.1)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->auto-gptq) (2024.2.0)\n",
      "Requirement already satisfied: ipywidgets==8.1.5 in /usr/local/lib/python3.10/dist-packages (8.1.5)\n",
      "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from ipywidgets==8.1.5) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets==8.1.5) (7.34.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets==8.1.5) (5.7.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /usr/local/lib/python3.10/dist-packages (from ipywidgets==8.1.5) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /usr/local/lib/python3.10/dist-packages (from ipywidgets==8.1.5) (3.0.13)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets==8.1.5) (75.1.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets==8.1.5) (0.19.2)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets==8.1.5) (4.4.2)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets==8.1.5) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets==8.1.5) (3.0.48)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets==8.1.5) (2.19.1)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets==8.1.5) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets==8.1.5) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets==8.1.5) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets==8.1.5) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets==8.1.5) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets==8.1.5) (0.2.13)\n",
      "Name: optimum\n",
      "Version: 1.24.0\n",
      "Summary: Optimum Library is an extension of the Hugging Face Transformers library, providing a framework to integrate third-party libraries from Hardware Partners and interface with their specific functionality.\n",
      "Home-page: https://github.com/huggingface/optimum\n",
      "Author: HuggingFace Inc. Special Ops Team\n",
      "Author-email: hardware@huggingface.co\n",
      "License: Apache\n",
      "Location: /usr/local/lib/python3.10/dist-packages\n",
      "Requires: huggingface-hub, numpy, packaging, torch, transformers\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip install datasets==2.15.0\n",
    "\n",
    "!pip install trl\n",
    "\n",
    "!pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu117/\n",
    "\n",
    "!pip install ipywidgets==8.1.5\n",
    "\n",
    "!pip show optimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WqTPRC5fI8re"
   },
   "source": [
    "# Load model and tokenizer from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T15:38:09.892195Z",
     "iopub.status.busy": "2025-03-05T15:38:09.891886Z",
     "iopub.status.idle": "2025-03-05T15:38:10.630237Z",
     "shell.execute_reply": "2025-03-05T15:38:10.629422Z",
     "shell.execute_reply.started": "2025-03-05T15:38:09.892170Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "The token `Shadman114` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `Shadman114`\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token --Your Hugging-Face Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-03-05T15:38:10.631558Z",
     "iopub.status.busy": "2025-03-05T15:38:10.631289Z",
     "iopub.status.idle": "2025-03-05T15:38:29.639804Z",
     "shell.execute_reply": "2025-03-05T15:38:29.639105Z",
     "shell.execute_reply.started": "2025-03-05T15:38:10.631535Z"
    },
    "executionInfo": {
     "elapsed": 7700,
     "status": "ok",
     "timestamp": 1715500264403,
     "user": {
      "displayName": "Shree Krishna Jamakatel",
      "userId": "14328212804148880429"
     },
     "user_tz": -345
    },
    "id": "lIw4uj3gI7e7",
    "outputId": "6b1b8bfb-fb27-4c4c-bc28-3fdfd14f16ac",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf7efd22b2f949aa86606e8de64e603d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/40.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f22e7f3c044fd1874e18b7de8e4d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f17eef503694b5193433334e6091263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9987b76be55e4f65969f458d524e68a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GPTQConfig\n",
    "\n",
    "#model_id = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\n",
    "model_id = \"marcsun13/gemma-2-9b-it-GPTQ\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XmWqEPLInoVr"
   },
   "source": [
    "# Load RAFT dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T15:38:29.641122Z",
     "iopub.status.busy": "2025-03-05T15:38:29.640533Z",
     "iopub.status.idle": "2025-03-05T15:38:30.118013Z",
     "shell.execute_reply": "2025-03-05T15:38:30.117323Z",
     "shell.execute_reply.started": "2025-03-05T15:38:29.641072Z"
    },
    "id": "3Y16F58cJGJS",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset_path = \"/kaggle/input/raft-train-dataset/data-00000-of-00001.arrow\"\n",
    "\n",
    "ds = Dataset.from_file(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-03-05T15:38:30.119531Z",
     "iopub.status.busy": "2025-03-05T15:38:30.118889Z",
     "iopub.status.idle": "2025-03-05T15:38:30.124640Z",
     "shell.execute_reply": "2025-03-05T15:38:30.123853Z",
     "shell.execute_reply.started": "2025-03-05T15:38:30.119492Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1715500297249,
     "user": {
      "displayName": "Shree Krishna Jamakatel",
      "userId": "14328212804148880429"
     },
     "user_tz": -345
    },
    "id": "edpgzJIWKx5U",
    "outputId": "83823bcb-ffae-42a2-d9e6-a8802234eac4",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'type', 'question', 'context', 'oracle_context', 'cot_answer', 'instruction'],\n",
       "    num_rows: 240\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "execution": {
     "iopub.execute_input": "2025-03-05T15:38:30.125709Z",
     "iopub.status.busy": "2025-03-05T15:38:30.125461Z",
     "iopub.status.idle": "2025-03-05T15:38:30.216621Z",
     "shell.execute_reply": "2025-03-05T15:38:30.215766Z",
     "shell.execute_reply.started": "2025-03-05T15:38:30.125688Z"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1715500297941,
     "user": {
      "displayName": "Shree Krishna Jamakatel",
      "userId": "14328212804148880429"
     },
     "user_tz": -345
    },
    "id": "LSLczdzznt-m",
    "outputId": "680ffe4f-729a-4eb1-9b8b-955a4854b790",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>oracle_context</th>\n",
       "      <th>cot_answer</th>\n",
       "      <th>instruction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>seed_task_0</td>\n",
       "      <td>general</td>\n",
       "      <td>What is a Megger?</td>\n",
       "      <td>{'sentences': [['The word “Megger” is a regist...</td>\n",
       "      <td>The word “Megger” is a registered trademark\\nW...</td>\n",
       "      <td>assistant: To answer the question \"What is a M...</td>\n",
       "      <td>&lt;DOCUMENT&gt;The word “Megger” is a registered tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>seed_task_1</td>\n",
       "      <td>general</td>\n",
       "      <td>It seems there was no specific context provide...</td>\n",
       "      <td>{'sentences': [['it includes \n",
       "inspection, test...</td>\n",
       "      <td></td>\n",
       "      <td>assistant: Since there is no specific context ...</td>\n",
       "      <td>&lt;DOCUMENT&gt;it includes \\ninspection, testing, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>seed_task_2</td>\n",
       "      <td>general</td>\n",
       "      <td>What is the purpose of circuit breaker testing?</td>\n",
       "      <td>{'sentences': [['CirCuiT Breaker TesTing guide...</td>\n",
       "      <td>CirCuiT Breaker TesTing guide  3</td>\n",
       "      <td>assistant: To answer the question about the pu...</td>\n",
       "      <td>&lt;DOCUMENT&gt;CirCuiT Breaker TesTing guide  3&lt;/DO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>seed_task_3</td>\n",
       "      <td>general</td>\n",
       "      <td>What are the main types of circuit breakers me...</td>\n",
       "      <td>{'sentences': [['Of  interest is the curve [4-...</td>\n",
       "      <td>Contents\\nIntroduction ..........................</td>\n",
       "      <td>assistant: To answer the question about the ma...</td>\n",
       "      <td>&lt;DOCUMENT&gt;Of  interest is the curve [4-5]. As ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>seed_task_4</td>\n",
       "      <td>general</td>\n",
       "      <td>What are some error sources in testing?</td>\n",
       "      <td>{'sentences': [['Good to know about error sour...</td>\n",
       "      <td>Good to know about error sources ............ ...</td>\n",
       "      <td>assistant: To answer the question about error ...</td>\n",
       "      <td>&lt;DOCUMENT&gt;Good to know about error sources ......</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id     type                                           question  \\\n",
       "0  seed_task_0  general                                  What is a Megger?   \n",
       "1  seed_task_1  general  It seems there was no specific context provide...   \n",
       "2  seed_task_2  general    What is the purpose of circuit breaker testing?   \n",
       "3  seed_task_3  general  What are the main types of circuit breakers me...   \n",
       "4  seed_task_4  general            What are some error sources in testing?   \n",
       "\n",
       "                                             context  \\\n",
       "0  {'sentences': [['The word “Megger” is a regist...   \n",
       "1  {'sentences': [['it includes \n",
       "inspection, test...   \n",
       "2  {'sentences': [['CirCuiT Breaker TesTing guide...   \n",
       "3  {'sentences': [['Of  interest is the curve [4-...   \n",
       "4  {'sentences': [['Good to know about error sour...   \n",
       "\n",
       "                                      oracle_context  \\\n",
       "0  The word “Megger” is a registered trademark\\nW...   \n",
       "1                                                      \n",
       "2                   CirCuiT Breaker TesTing guide  3   \n",
       "3  Contents\\nIntroduction ..........................   \n",
       "4  Good to know about error sources ............ ...   \n",
       "\n",
       "                                          cot_answer  \\\n",
       "0  assistant: To answer the question \"What is a M...   \n",
       "1  assistant: Since there is no specific context ...   \n",
       "2  assistant: To answer the question about the pu...   \n",
       "3  assistant: To answer the question about the ma...   \n",
       "4  assistant: To answer the question about error ...   \n",
       "\n",
       "                                         instruction  \n",
       "0  <DOCUMENT>The word “Megger” is a registered tr...  \n",
       "1  <DOCUMENT>it includes \\ninspection, testing, o...  \n",
       "2  <DOCUMENT>CirCuiT Breaker TesTing guide  3</DO...  \n",
       "3  <DOCUMENT>Of  interest is the curve [4-5]. As ...  \n",
       "4  <DOCUMENT>Good to know about error sources ......  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(ds)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T15:38:30.219423Z",
     "iopub.status.busy": "2025-03-05T15:38:30.219191Z",
     "iopub.status.idle": "2025-03-05T15:38:30.226040Z",
     "shell.execute_reply": "2025-03-05T15:38:30.225225Z",
     "shell.execute_reply.started": "2025-03-05T15:38:30.219404Z"
    },
    "id": "6k7B_W7KnyEV",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Combine 'user' and 'assistant' columns into a new 'messages' column as list of dictionaries\n",
    "\n",
    "df['messages'] = df.apply(lambda row: [{'content': row['instruction'], 'role': 'user'},\n",
    "\n",
    "                                       {'content': row['cot_answer'], 'role': 'assistant'}], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-03-05T15:38:30.227221Z",
     "iopub.status.busy": "2025-03-05T15:38:30.226954Z",
     "iopub.status.idle": "2025-03-05T15:38:30.269692Z",
     "shell.execute_reply": "2025-03-05T15:38:30.268957Z",
     "shell.execute_reply.started": "2025-03-05T15:38:30.227196Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1715500297941,
     "user": {
      "displayName": "Shree Krishna Jamakatel",
      "userId": "14328212804148880429"
     },
     "user_tz": -345
    },
    "id": "kTLRZeInn1vk",
    "outputId": "46ed56a4-3006-455a-9e3f-451ff78f6a02",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'type', 'question', 'context', 'oracle_context', 'cot_answer', 'instruction', 'messages'],\n",
       "    num_rows: 240\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "ds = datasets.Dataset.from_pandas(df)\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20CcCz3Cn8Kl"
   },
   "source": [
    "# Chat Template for prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T15:38:30.270692Z",
     "iopub.status.busy": "2025-03-05T15:38:30.270443Z",
     "iopub.status.idle": "2025-03-05T15:38:30.274008Z",
     "shell.execute_reply": "2025-03-05T15:38:30.273258Z",
     "shell.execute_reply.started": "2025-03-05T15:38:30.270664Z"
    },
    "id": "tE5e9q5xL2qc",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set chat template\n",
    "\n",
    "DEFAULT_CHAT_TEMPLATE = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"\n",
    "\n",
    "tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "9140231431254912b95bb9947a13a78d",
      "47ad4f27a4224d37b81c99e0f13c5c3e",
      "243ee195c42c4215b7fb668dfb5a1567",
      "92331f58c5164dfeba3429718d06f284",
      "6d4ea37f7e4b4ae193094737f1d2de5d",
      "4446b0292b814bc29d8753711f578548",
      "83eff657fe884153ae4f5b6a53e17bba",
      "c61054da71a94bacad89e4acc1224dd2",
      "01091e38e381443589bd34b55c9f302c",
      "a9ff350a5c2b44deaedcf9d6094f654b",
      "e1f8785e74de4938ab6fd467110f7be4"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-03-05T15:38:30.275057Z",
     "iopub.status.busy": "2025-03-05T15:38:30.274831Z",
     "iopub.status.idle": "2025-03-05T15:38:35.524304Z",
     "shell.execute_reply": "2025-03-05T15:38:35.523317Z",
     "shell.execute_reply.started": "2025-03-05T15:38:30.275037Z"
    },
    "executionInfo": {
     "elapsed": 877,
     "status": "ok",
     "timestamp": 1715500299717,
     "user": {
      "displayName": "Shree Krishna Jamakatel",
      "userId": "14328212804148880429"
     },
     "user_tz": -345
    },
    "id": "NRsvb6f4doyl",
    "outputId": "63c5ff95-037f-4211-97e6-cadd7190eb17",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "855393e78881444bbb4a65bece1bed86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template (num_proc=4):   0%|          | 0/240 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import random\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "\n",
    "\n",
    "def apply_chat_template(example, tokenizer):\n",
    "\n",
    "    messages = example[\"messages\"]\n",
    "\n",
    "    # We add an empty system message if there is none\n",
    "\n",
    "    if messages[0][\"role\"] != \"system\":\n",
    "\n",
    "        messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
    "\n",
    "    example[\"text\"] = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "\n",
    "\n",
    "    return example\n",
    "\n",
    "\n",
    "\n",
    "column_names = list(ds.features)\n",
    "\n",
    "raw_datasets = ds.map(apply_chat_template,\n",
    "\n",
    "                                num_proc=cpu_count(),\n",
    "\n",
    "                                fn_kwargs={\"tokenizer\": tokenizer},\n",
    "\n",
    "                                remove_columns=column_names,\n",
    "\n",
    "                                desc=\"Applying chat template\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-03-05T15:38:35.525677Z",
     "iopub.status.busy": "2025-03-05T15:38:35.525399Z",
     "iopub.status.idle": "2025-03-05T15:38:35.531021Z",
     "shell.execute_reply": "2025-03-05T15:38:35.530427Z",
     "shell.execute_reply.started": "2025-03-05T15:38:35.525642Z"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1715500299717,
     "user": {
      "displayName": "Shree Krishna Jamakatel",
      "userId": "14328212804148880429"
     },
     "user_tz": -345
    },
    "id": "A83d-JwXhRV8",
    "outputId": "8c43c319-cd8d-4d4d-a4d6-32a6b256eeb6",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 240\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "execution": {
     "iopub.execute_input": "2025-03-05T15:38:35.531912Z",
     "iopub.status.busy": "2025-03-05T15:38:35.531703Z",
     "iopub.status.idle": "2025-03-05T15:38:35.551262Z",
     "shell.execute_reply": "2025-03-05T15:38:35.550232Z",
     "shell.execute_reply.started": "2025-03-05T15:38:35.531884Z"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1715500299717,
     "user": {
      "displayName": "Shree Krishna Jamakatel",
      "userId": "14328212804148880429"
     },
     "user_tz": -345
    },
    "id": "Wk04Bv7ph2_1",
    "outputId": "eb1d8249-9046-45c9-c51e-8881177b12b4",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|system|>\\n<eos>\\n<|user|>\\n<DOCUMENT>The word “Megger” is a registered trademark\\nWWW.MEGGER.COM\\n▪▪ Circuit breaker function and test methods\\n▪▪ Practical circuit breaker testing\\n▪▪ Megger CB testing products overview\\nCircuit Breaker \\ntesting guide</DOCUMENT>\\n<DOCUMENT>over time will fi ll-up with water or water vapor. Hence, water filled v oids in the \\nextruded insulation are known as water trees because th ese voids when inspected \\nunder a microscope resemble like a tree . Studies indicated that water treeing is the \\ncrucial degradation form that may affl ict older XLPE and high -molecular weight PE -\\nextruded cables. Water treeing is self-propagating dendritic pattern of electro -\\noxidation, which decreases extruded insulation AC and impulse breakdown \\nstrengths. It is the main degradation mechanism of extruded medium-voltage power \\ncables. Even though carefully studied, the initiation and growth mechanisms of water \\ntreeing are not yet clear. </DOCUMENT>\\nWhat is a Megger?<eos>\\n<|assistant|>\\nassistant: To answer the question \"What is a Megger?\", we need to look at the context provided. The context gives us a clue about the nature of \"Megger\" by mentioning it as a registered trademark. It also provides a website, www.megger.com, which is likely the official site for the brand. The context further elaborates on products and services related to electrical testing, specifically mentioning circuit breaker testing products. However, the context does not provide a direct definition of what a Megger is.\\n\\n##begin_quote##\\nThe word “Megger” is a registered trademark\\n##end_quote##\\n\\nFrom this, we can infer that \"Megger\" is a brand name, likely associated with electrical testing equipment. The context suggests that the company produces various testing devices, particularly for circuit breakers, which implies that a Megger is a type of testing device or a brand of testing devices.\\n\\n<ANSWER>: A Megger is a registered trademark for a brand of electrical testing equipment, particularly known for its circuit breaker testing products.<eos>\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"text\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T15:38:35.552365Z",
     "iopub.status.busy": "2025-03-05T15:38:35.552073Z",
     "iopub.status.idle": "2025-03-05T15:38:35.632127Z",
     "shell.execute_reply": "2025-03-05T15:38:35.631277Z",
     "shell.execute_reply.started": "2025-03-05T15:38:35.552344Z"
    },
    "id": "aO_2QdVPhZP5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "raw_datasets = raw_datasets.train_test_split(test_size=0.1)\n",
    "\n",
    "# create the splits\n",
    "\n",
    "train_dataset = raw_datasets[\"train\"]\n",
    "\n",
    "eval_dataset = raw_datasets[\"test\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FiYyCkqDoCX6"
   },
   "source": [
    "# Peft config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T15:38:35.633276Z",
     "iopub.status.busy": "2025-03-05T15:38:35.632997Z",
     "iopub.status.idle": "2025-03-05T15:38:35.638901Z",
     "shell.execute_reply": "2025-03-05T15:38:35.638162Z",
     "shell.execute_reply.started": "2025-03-05T15:38:35.633255Z"
    },
    "id": "E8Ens6sLfBQO",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "\n",
    "    r=8,\n",
    "\n",
    "    lora_alpha=32,\n",
    "\n",
    "    target_modules=[\"k_proj\",\"o_proj\",\"q_proj\",\"v_proj\"],\n",
    "\n",
    "    lora_dropout=0.05,\n",
    "\n",
    "    bias=\"none\",\n",
    "\n",
    "    task_type=\"CAUSAL_LM\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "guep1rnsoHMB"
   },
   "source": [
    "# Quantization Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-03-05T15:38:35.640033Z",
     "iopub.status.busy": "2025-03-05T15:38:35.639750Z",
     "iopub.status.idle": "2025-03-05T15:38:35.656183Z",
     "shell.execute_reply": "2025-03-05T15:38:35.655500Z",
     "shell.execute_reply.started": "2025-03-05T15:38:35.640012Z"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1715500299717,
     "user": {
      "displayName": "Shree Krishna Jamakatel",
      "userId": "14328212804148880429"
     },
     "user_tz": -345
    },
    "id": "0ADAN4P8emLD",
    "outputId": "539d086b-27af-4918-e0b7-8024d9f3e444",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `disable_exllama` is deprecated and will be removed in version 4.37. Use `use_exllama` instead and specify the version with `exllama_config`.The value of `use_exllama` will be overwritten by `disable_exllama` passed in `GPTQConfig` or stored in your config file.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPTQConfig\n",
    "\n",
    "\n",
    "\n",
    "quantization_config = GPTQConfig(bits=4, disable_exllama=True)\n",
    "\n",
    "\n",
    "\n",
    "model_kwargs = dict(\n",
    "\n",
    "    torch_dtype=\"auto\",\n",
    "\n",
    "    use_cache=False, # set to False as we're going to use gradient checkpointing\n",
    "\n",
    "    device_map=\"auto\",\n",
    "\n",
    "    quantization_config=quantization_config,\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnYsRkqkoK7i"
   },
   "source": [
    "# SFT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T15:41:01.956728Z",
     "iopub.status.busy": "2025-03-05T15:41:01.956431Z",
     "iopub.status.idle": "2025-03-05T15:41:04.854235Z",
     "shell.execute_reply": "2025-03-05T15:41:04.853359Z",
     "shell.execute_reply.started": "2025-03-05T15:41:01.956706Z"
    },
    "id": "gJu15-EVh5mW",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "from peft import LoraConfig\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "\n",
    "# path where the Trainer will save its checkpoints and logs\n",
    "\n",
    "output_dir = 'raft-sft-output'\n",
    "\n",
    "\n",
    "\n",
    "# based on config\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "\n",
    "    fp16=True, # specify bf16=True instead when training on GPUs that support bf16\n",
    "\n",
    "    do_eval=True,\n",
    "\n",
    "    evaluation_strategy=\"epoch\",\n",
    "\n",
    "    gradient_accumulation_steps=128,\n",
    "\n",
    "    gradient_checkpointing=True,\n",
    "\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "\n",
    "    learning_rate=2.0e-05,\n",
    "\n",
    "    log_level=\"info\",\n",
    "\n",
    "    logging_steps=5,\n",
    "\n",
    "    logging_strategy=\"steps\",\n",
    "\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "\n",
    "    max_steps=-1,\n",
    "\n",
    "    num_train_epochs=4,\n",
    "\n",
    "    output_dir=output_dir,\n",
    "\n",
    "    overwrite_output_dir=True,\n",
    "\n",
    "    per_device_eval_batch_size=1, # originally set to 8\n",
    "\n",
    "    per_device_train_batch_size=1, # originally set to 8\n",
    "\n",
    "    save_strategy=\"no\",\n",
    "\n",
    "    save_total_limit=None,\n",
    "\n",
    "    seed=42,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 327,
     "referenced_widgets": [
      "3468ea53e18f487c904db761a3151e26",
      "d2c9e2725c4945a5b964d9b8d320a60f",
      "8f52957ef82f41c3b324c3f5a7a5786b",
      "40027e9711b94dbe9cc21d3741987027",
      "99b76ece52204ad19a3ee1aa620b2e1b",
      "b4d845cb25ea40638c08cd183ca33f33",
      "1c2c818714c94a7aae16980c8a0a37e0",
      "4e72a5792bc54b73a98860c4621b41df",
      "a262e7766edd4b5c91d1a84561b1ce40",
      "7284c45c19144968b66a07227f6f4009",
      "c37f39d8708745678e213733a5bad77b",
      "20ebb1fd076244a4a01799368ed95aea",
      "ac93a658d6f740a2a14c17505b3b1d69",
      "ddd757cdc16242b1852c42dadaadbf55",
      "2e482407d0844657ae76ee0d335bf54d",
      "1ec9e18c3e7a4e2b944ae1508b4ebc42",
      "e4af32265d5146ab8399ccea8c6344f1",
      "1a80ee9fd8954ee8bac710aac905d337",
      "813115b671be4e3699b74dbfb49ee6cd",
      "f8ebc6a91fbf41dfa9fb91bddd06b885",
      "f3106d961547450aa23af894e235e57c",
      "a7f27888df0f406da70c0cb26701b3c3"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-03-05T15:41:19.842851Z",
     "iopub.status.busy": "2025-03-05T15:41:19.842548Z",
     "iopub.status.idle": "2025-03-05T15:41:54.496492Z",
     "shell.execute_reply": "2025-03-05T15:41:54.495824Z",
     "shell.execute_reply.started": "2025-03-05T15:41:19.842827Z"
    },
    "executionInfo": {
     "elapsed": 10549,
     "status": "ok",
     "timestamp": 1715500384874,
     "user": {
      "displayName": "Shree Krishna Jamakatel",
      "userId": "14328212804148880429"
     },
     "user_tz": -345
    },
    "id": "2aYnguMGlkNh",
    "outputId": "4fa59420-bb12-410c-b100-48ebde4e47cc",
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-9b20d650f9d0>:1: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d10d9814c2c547a68aa8158dc9bbc0a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.50k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fb5bf723e234a4e90ca32b164d0f737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/134k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4134135100194cc38d2fcf916b2c0128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d88f7c08072b4ad9a3c837de498c4688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd82334272b24030b3c6de35edf29c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.19G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have set `use_cache` to `False`, but cache_implementation is set to hybrid. cache_implementation will have no effect.\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b530f8ebf204579b2ede9e04bd811ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at marcsun13/gemma-2-9b-it-GPTQ were not used when initializing Gemma2ForCausalLM: {'model.layers.35.self_attn.k_proj.bias', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.38.mlp.up_proj.bias', 'model.layers.41.self_attn.k_proj.bias', 'model.layers.33.mlp.gate_proj.bias', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.20.mlp.gate_proj.bias', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.31.mlp.up_proj.bias', 'model.layers.7.mlp.gate_proj.bias', 'model.layers.41.mlp.gate_proj.bias', 'model.layers.33.self_attn.o_proj.bias', 'model.layers.24.mlp.up_proj.bias', 'model.layers.14.self_attn.o_proj.bias', 'model.layers.11.mlp.up_proj.bias', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.25.mlp.down_proj.bias', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.40.mlp.up_proj.bias', 'model.layers.21.mlp.gate_proj.bias', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.38.mlp.down_proj.bias', 'model.layers.34.mlp.gate_proj.bias', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.3.mlp.gate_proj.bias', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.36.self_attn.v_proj.bias', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.39.self_attn.k_proj.bias', 'model.layers.32.self_attn.v_proj.bias', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.40.self_attn.o_proj.bias', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.0.mlp.gate_proj.bias', 'model.layers.12.mlp.down_proj.bias', 'model.layers.29.mlp.up_proj.bias', 'model.layers.33.self_attn.k_proj.bias', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.22.mlp.down_proj.bias', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.32.self_attn.k_proj.bias', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.28.mlp.gate_proj.bias', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.37.mlp.down_proj.bias', 'model.layers.7.mlp.up_proj.bias', 'model.layers.38.self_attn.k_proj.bias', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.33.mlp.down_proj.bias', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.9.mlp.up_proj.bias', 'model.layers.27.mlp.gate_proj.bias', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.34.self_attn.k_proj.bias', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.26.mlp.gate_proj.bias', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.40.self_attn.v_proj.bias', 'model.layers.26.mlp.up_proj.bias', 'model.layers.9.mlp.gate_proj.bias', 'model.layers.39.self_attn.o_proj.bias', 'model.layers.10.mlp.up_proj.bias', 'model.layers.25.mlp.up_proj.bias', 'model.layers.25.self_attn.o_proj.bias', 'model.layers.34.self_attn.q_proj.bias', 'model.layers.32.self_attn.o_proj.bias', 'model.layers.8.mlp.down_proj.bias', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.5.mlp.up_proj.bias', 'model.layers.19.mlp.down_proj.bias', 'model.layers.8.mlp.up_proj.bias', 'model.layers.30.self_attn.o_proj.bias', 'model.layers.5.mlp.gate_proj.bias', 'model.layers.0.mlp.up_proj.bias', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.6.mlp.up_proj.bias', 'model.layers.4.mlp.down_proj.bias', 'model.layers.27.mlp.up_proj.bias', 'model.layers.18.mlp.down_proj.bias', 'model.layers.24.self_attn.o_proj.bias', 'model.layers.17.mlp.gate_proj.bias', 'model.layers.2.mlp.up_proj.bias', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.4.mlp.gate_proj.bias', 'model.layers.8.mlp.gate_proj.bias', 'model.layers.21.mlp.down_proj.bias', 'model.layers.15.mlp.up_proj.bias', 'model.layers.2.mlp.down_proj.bias', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.16.mlp.gate_proj.bias', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.22.mlp.gate_proj.bias', 'model.layers.24.mlp.down_proj.bias', 'model.layers.29.mlp.gate_proj.bias', 'model.layers.28.self_attn.o_proj.bias', 'model.layers.33.self_attn.q_proj.bias', 'model.layers.32.mlp.up_proj.bias', 'model.layers.19.mlp.up_proj.bias', 'model.layers.27.mlp.down_proj.bias', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.32.mlp.gate_proj.bias', 'model.layers.37.self_attn.o_proj.bias', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.36.mlp.down_proj.bias', 'model.layers.12.mlp.gate_proj.bias', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.38.self_attn.q_proj.bias', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.35.self_attn.v_proj.bias', 'model.layers.12.mlp.up_proj.bias', 'model.layers.34.mlp.down_proj.bias', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.1.mlp.up_proj.bias', 'model.layers.40.mlp.gate_proj.bias', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.33.mlp.up_proj.bias', 'model.layers.16.mlp.up_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.41.self_attn.v_proj.bias', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.17.mlp.down_proj.bias', 'model.layers.36.mlp.up_proj.bias', 'model.layers.31.mlp.gate_proj.bias', 'model.layers.3.mlp.down_proj.bias', 'model.layers.37.mlp.gate_proj.bias', 'model.layers.21.mlp.up_proj.bias', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.14.mlp.up_proj.bias', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.22.self_attn.o_proj.bias', 'model.layers.34.mlp.up_proj.bias', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.30.mlp.gate_proj.bias', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.30.mlp.down_proj.bias', 'model.layers.36.self_attn.k_proj.bias', 'model.layers.37.self_attn.q_proj.bias', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.14.mlp.down_proj.bias', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.34.self_attn.o_proj.bias', 'model.layers.39.self_attn.q_proj.bias', 'model.layers.17.self_attn.o_proj.bias', 'model.layers.32.mlp.down_proj.bias', 'model.layers.40.self_attn.k_proj.bias', 'model.layers.39.mlp.gate_proj.bias', 'model.layers.32.self_attn.q_proj.bias', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.13.mlp.gate_proj.bias', 'model.layers.13.mlp.up_proj.bias', 'model.layers.25.mlp.gate_proj.bias', 'model.layers.20.mlp.up_proj.bias', 'model.layers.15.mlp.down_proj.bias', 'model.layers.26.self_attn.o_proj.bias', 'model.layers.39.mlp.down_proj.bias', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.10.mlp.gate_proj.bias', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.34.self_attn.v_proj.bias', 'model.layers.38.self_attn.o_proj.bias', 'model.layers.9.mlp.down_proj.bias', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.29.self_attn.o_proj.bias', 'model.layers.3.mlp.up_proj.bias', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.35.mlp.gate_proj.bias', 'model.layers.16.mlp.down_proj.bias', 'model.layers.37.self_attn.v_proj.bias', 'model.layers.39.self_attn.v_proj.bias', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.38.self_attn.v_proj.bias', 'model.layers.36.mlp.gate_proj.bias', 'model.layers.28.mlp.up_proj.bias', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.11.mlp.down_proj.bias', 'model.layers.1.mlp.down_proj.bias', 'model.layers.31.self_attn.o_proj.bias', 'model.layers.41.self_attn.q_proj.bias', 'model.layers.41.mlp.down_proj.bias', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.0.mlp.down_proj.bias', 'model.layers.18.mlp.gate_proj.bias', 'model.layers.9.self_attn.o_proj.bias', 'model.layers.39.mlp.up_proj.bias', 'model.layers.30.mlp.up_proj.bias', 'model.layers.17.mlp.up_proj.bias', 'model.layers.13.mlp.down_proj.bias', 'model.layers.11.mlp.gate_proj.bias', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.10.mlp.down_proj.bias', 'model.layers.7.mlp.down_proj.bias', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.23.mlp.up_proj.bias', 'model.layers.2.mlp.gate_proj.bias', 'model.layers.6.mlp.down_proj.bias', 'model.layers.41.self_attn.o_proj.bias', 'model.layers.18.mlp.up_proj.bias', 'model.layers.5.mlp.down_proj.bias', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.20.mlp.down_proj.bias', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.27.self_attn.o_proj.bias', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.15.mlp.gate_proj.bias', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.40.self_attn.q_proj.bias', 'model.layers.4.mlp.up_proj.bias', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.22.mlp.up_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.23.self_attn.o_proj.bias', 'model.layers.23.mlp.gate_proj.bias', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.33.self_attn.v_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.40.mlp.down_proj.bias', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.6.mlp.gate_proj.bias', 'model.layers.36.self_attn.o_proj.bias', 'model.layers.26.mlp.down_proj.bias', 'model.layers.37.mlp.up_proj.bias', 'model.layers.28.mlp.down_proj.bias', 'model.layers.31.mlp.down_proj.bias', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.36.self_attn.q_proj.bias', 'model.layers.35.mlp.up_proj.bias', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.23.mlp.down_proj.bias', 'model.layers.24.mlp.gate_proj.bias', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.35.self_attn.o_proj.bias', 'model.layers.38.mlp.gate_proj.bias', 'model.layers.14.mlp.gate_proj.bias', 'model.layers.29.mlp.down_proj.bias', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.37.self_attn.k_proj.bias', 'model.layers.41.mlp.up_proj.bias', 'model.layers.1.mlp.gate_proj.bias', 'model.layers.35.self_attn.q_proj.bias', 'model.layers.19.mlp.gate_proj.bias', 'model.layers.35.mlp.down_proj.bias', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.23.self_attn.k_proj.bias'}\n",
      "- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "013375537181402e9bc5a232cbe9f29a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "960e4c840b374d5bba89461684412433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/216 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c1eb92c14e34de9b5d2d5894d3e8db8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/216 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99c9efe0dbae437784fc47a2aa102d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/216 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cde4e9479574e64b708726b3c46cc7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/216 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05b1f5b18f58408c8882f16bc77e7df3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting eval dataset to ChatML:   0%|          | 0/24 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e525300cc5894706923a62fee10266f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset:   0%|          | 0/24 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c35a224f501499593b733a2c37a0f41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/24 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c638bb2f5e4e4d08a652e18cf92312c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/24 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have loaded a model on multiple GPUs. `is_model_parallel` attribute will be force-set to `True` to avoid any unexpected behavior such as device placement mismatching.\n",
      "Using auto half precision backend\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "\n",
    "        model=model_id,\n",
    "\n",
    "        #model_init_kwargs=model_kwargs,\n",
    "\n",
    "        args=training_args,\n",
    "\n",
    "        train_dataset=train_dataset,\n",
    "\n",
    "        eval_dataset=eval_dataset,\n",
    "\n",
    "        #dataset_text_field=\"text\",\n",
    "\n",
    "        tokenizer=tokenizer,\n",
    "\n",
    "        # packing=True,\n",
    "\n",
    "        peft_config=peft_config,\n",
    "\n",
    "        # max_seq_length=tokenizer.model_max_length,\n",
    "\n",
    "        # max_seq_length=512,\n",
    "\n",
    "        #max_seq_length=2048\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting pad_token=eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T15:41:54.497726Z",
     "iopub.status.busy": "2025-03-05T15:41:54.497459Z",
     "iopub.status.idle": "2025-03-05T15:41:54.500865Z",
     "shell.execute_reply": "2025-03-05T15:41:54.500234Z",
     "shell.execute_reply.started": "2025-03-05T15:41:54.497703Z"
    },
    "id": "u7nB0XEKiP3w",
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T15:41:54.502300Z",
     "iopub.status.busy": "2025-03-05T15:41:54.502045Z",
     "iopub.status.idle": "2025-03-05T15:41:59.585895Z",
     "shell.execute_reply": "2025-03-05T15:41:59.584826Z",
     "shell.execute_reply.started": "2025-03-05T15:41:54.502279Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.49.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.2)\n",
      "Requirement already satisfied: trl in /usr/local/lib/python3.10/dist-packages (0.15.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.1+cu121)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl) (13.9.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers accelerate peft datasets trl\n",
    "\n",
    "import json\n",
    "from transformers import TrainingArguments, TrainerCallback, TrainerState, TrainerControl\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "\n",
    "class CustomTensorBoardCallback(TensorBoardCallback):\n",
    "    \"\"\"\n",
    "    Custom TensorBoard callback that handles non-serializable objects in TrainingArguments.\n",
    "    \"\"\"\n",
    "    def on_train_begin(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        \"\"\"\n",
    "        Event called at the beginning of training.\n",
    "\n",
    "        Overridden to handle non-serializable objects in TrainingArguments.\n",
    "        \"\"\"\n",
    "        if self.tb_writer is not None:\n",
    "            # Convert args to a dictionary and remove non-serializable objects\n",
    "            args_dict = args.to_dict()\n",
    "\n",
    "            # Iterate through the dictionary and remove non-serializable objects\n",
    "            for key, value in list(args_dict.items()):\n",
    "                if not isinstance(value,(int,float,str,bool,list,tuple,dict,type(None))):\n",
    "                    del args_dict[key]\n",
    "\n",
    "            self.tb_writer.add_text(\"args\", json.dumps(args_dict, indent=2))\n",
    "\n",
    "            if \"model\" in kwargs:\n",
    "                model = kwargs[\"model\"]\n",
    "                if hasattr(model, \"config\") and model.config is not None:\n",
    "                    model_config_json = model.config.to_json_string()\n",
    "                    self.tb_writer.add_text(\"model_config\", model_config_json)\n",
    "\n",
    "\n",
    "# Replace the default TensorBoardCallback with the custom one\n",
    "trainer.remove_callback(TensorBoardCallback)  # Remove default if present\n",
    "trainer.add_callback(CustomTensorBoardCallback)  # Add custom callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wandb  168899a11d1a94cd8fd80d8f2d31623ab2a485ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T15:43:51.769038Z",
     "iopub.status.busy": "2025-03-05T15:43:51.768708Z",
     "iopub.status.idle": "2025-03-05T15:43:51.772842Z",
     "shell.execute_reply": "2025-03-05T15:43:51.771949Z",
     "shell.execute_reply.started": "2025-03-05T15:43:51.769009Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os\n",
    "os.environ[\"WANDB_API_KEY\"] = \"168899a11d1a94cd8fd80d8f2d31623ab2a485ab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T15:44:35.999579Z",
     "iopub.status.busy": "2025-03-05T15:44:35.999279Z",
     "iopub.status.idle": "2025-03-05T15:44:36.136265Z",
     "shell.execute_reply": "2025-03-05T15:44:36.135563Z",
     "shell.execute_reply.started": "2025-03-05T15:44:35.999555Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(key=\"168899a11d1a94cd8fd80d8f2d31623ab2a485ab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "execution": {
     "iopub.execute_input": "2025-03-05T15:44:39.143127Z",
     "iopub.status.busy": "2025-03-05T15:44:39.142804Z",
     "iopub.status.idle": "2025-03-05T16:49:16.791978Z",
     "shell.execute_reply": "2025-03-05T16:49:16.791151Z",
     "shell.execute_reply.started": "2025-03-05T15:44:39.143099Z"
    },
    "executionInfo": {
     "elapsed": 641852,
     "status": "ok",
     "timestamp": 1715501040775,
     "user": {
      "displayName": "Shree Krishna Jamakatel",
      "userId": "14328212804148880429"
     },
     "user_tz": -345
    },
    "id": "bvF45nSJkn9N",
    "outputId": "5f4b13ea-9d3c-4173-9f8e-82fb6bfc4151",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text. If text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 216\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 128\n",
      "  Total optimization steps = 4\n",
      "  Number of trainable parameters = 8,945,664\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250305_154441-yygiv7ah</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shadmansobhan114-bangladesh-university-of-engineering-an/huggingface/runs/yygiv7ah' target=\"_blank\">raft-sft-output</a></strong> to <a href='https://wandb.ai/shadmansobhan114-bangladesh-university-of-engineering-an/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shadmansobhan114-bangladesh-university-of-engineering-an/huggingface' target=\"_blank\">https://wandb.ai/shadmansobhan114-bangladesh-university-of-engineering-an/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shadmansobhan114-bangladesh-university-of-engineering-an/huggingface/runs/yygiv7ah' target=\"_blank\">https://wandb.ai/shadmansobhan114-bangladesh-university-of-engineering-an/huggingface/runs/yygiv7ah</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to train Gemma2 models with the `eager` attention implementation instead of `sdpa`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 46:01, Epoch 2/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.451845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.437057</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text. If text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 24\n",
      "  Batch size = 1\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text. If text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 24\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T16:49:42.817646Z",
     "iopub.status.busy": "2025-03-05T16:49:42.817222Z",
     "iopub.status.idle": "2025-03-05T16:49:42.822918Z",
     "shell.execute_reply": "2025-03-05T16:49:42.822152Z",
     "shell.execute_reply.started": "2025-03-05T16:49:42.817603Z"
    },
    "id": "96hUM_dvkBDT",
    "trusted": true
   },
   "outputs": [],
   "source": [
    " metrics = train_result.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T16:52:07.070910Z",
     "iopub.status.busy": "2025-03-05T16:52:07.070594Z",
     "iopub.status.idle": "2025-03-05T16:52:07.076779Z",
     "shell.execute_reply": "2025-03-05T16:52:07.075953Z",
     "shell.execute_reply.started": "2025-03-05T16:52:07.070887Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 3874.9089, 'train_samples_per_second': 0.223, 'train_steps_per_second': 0.001, 'total_flos': 17547070894080.0, 'train_loss': 2.418092727661133}\n"
     ]
    }
   ],
   "source": [
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the finetuned_raft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-03-05T16:53:52.075497Z",
     "iopub.status.busy": "2025-03-05T16:53:52.074934Z",
     "iopub.status.idle": "2025-03-05T16:53:53.047188Z",
     "shell.execute_reply": "2025-03-05T16:53:53.046282Z",
     "shell.execute_reply.started": "2025-03-05T16:53:52.075468Z"
    },
    "executionInfo": {
     "elapsed": 716,
     "status": "ok",
     "timestamp": 1715501047494,
     "user": {
      "displayName": "Shree Krishna Jamakatel",
      "userId": "14328212804148880429"
     },
     "user_tz": -345
    },
    "id": "MLvjew0KklD8",
    "outputId": "d0925479-005b-4e40-ba15-9ba2f25bf37a",
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to finetuned_raft_model\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--marcsun13--gemma-2-9b-it-GPTQ/snapshots/9c51e21691674594afc671d8edaeb93aa0cc9f7a/config.json\n",
      "Model config Gemma2Config {\n",
      "  \"_name_or_path\": \"google/gemma-2-9b-it-gptq\",\n",
      "  \"architectures\": [\n",
      "    \"Gemma2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": 50.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": 30.0,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_act\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"gemma2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 42,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"quantization_config\": {\n",
      "    \"batch_size\": 1,\n",
      "    \"bits\": 4,\n",
      "    \"block_name_to_quantize\": null,\n",
      "    \"cache_block_outputs\": true,\n",
      "    \"damp_percent\": 0.1,\n",
      "    \"dataset\": \"c4\",\n",
      "    \"desc_act\": false,\n",
      "    \"exllama_config\": {\n",
      "      \"version\": 2\n",
      "    },\n",
      "    \"group_size\": 128,\n",
      "    \"max_input_length\": null,\n",
      "    \"model_seqlen\": null,\n",
      "    \"module_name_preceding_first_block\": null,\n",
      "    \"modules_in_block_to_quantize\": null,\n",
      "    \"pad_token_id\": null,\n",
      "    \"quant_method\": \"gptq\",\n",
      "    \"sym\": true,\n",
      "    \"tokenizer\": null,\n",
      "    \"true_sequential\": true,\n",
      "    \"use_cuda_fp16\": false,\n",
      "    \"use_exllama\": true\n",
      "  },\n",
      "  \"query_pre_attn_scalar\": 224,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"sliding_window_size\": 4096,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 256000\n",
      "}\n",
      "\n",
      "tokenizer config file saved in finetuned_raft_model/tokenizer_config.json\n",
      "Special tokens file saved in finetuned_raft_model/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "output_dir_new = \"finetuned_raft_model\"\n",
    "\n",
    "trainer.save_model(output_dir_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T09:24:11.736748Z",
     "iopub.status.busy": "2024-11-14T09:24:11.736058Z",
     "iopub.status.idle": "2024-11-14T09:24:19.732339Z",
     "shell.execute_reply": "2024-11-14T09:24:19.731382Z",
     "shell.execute_reply.started": "2024-11-14T09:24:11.736693Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/working_directory.zip'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Define the directory to be zipped and the output zip file name\n",
    "directory_to_zip = '/kaggle/working/'\n",
    "output_zip_file = '/kaggle/working/working_directory.zip'\n",
    "\n",
    "# Zip the directory\n",
    "shutil.make_archive(output_zip_file.replace('.zip', ''), 'zip', directory_to_zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T16:54:00.086435Z",
     "iopub.status.busy": "2025-03-05T16:54:00.086064Z",
     "iopub.status.idle": "2025-03-05T16:54:00.091612Z",
     "shell.execute_reply": "2025-03-05T16:54:00.090728Z",
     "shell.execute_reply.started": "2025-03-05T16:54:00.086405Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "output_dir_new = \"finetuned_raft_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T14:51:51.119019Z",
     "iopub.status.busy": "2024-11-08T14:51:51.118353Z",
     "iopub.status.idle": "2024-11-08T14:51:51.127952Z",
     "shell.execute_reply": "2024-11-08T14:51:51.127008Z",
     "shell.execute_reply.started": "2024-11-08T14:51:51.118977Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of 'raft-sft-output': 0.00 MB\n",
      "Size of 'finetuned_raft_model': 71.03 MB\n",
      "Size of '.virtual_documents': 0.00 MB\n",
      "Size of 'wandb': 0.31 MB\n",
      "Size of 'finetunedGemma': 34.17 MB\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "\n",
    "# # Function to calculate the size of a folder\n",
    "# def get_folder_size(folder_path):\n",
    "#     total_size = 0\n",
    "#     for dirpath, dirnames, filenames in os.walk(folder_path):\n",
    "#         for f in filenames:\n",
    "#             fp = os.path.join(dirpath, f)\n",
    "#             total_size += os.path.getsize(fp)\n",
    "#     return total_size\n",
    "\n",
    "# # Path to the working directory\n",
    "# working_dir = '/kaggle/working/'\n",
    "\n",
    "# # Iterate over each subfolder in the working directory\n",
    "# for subfolder in os.listdir(working_dir):\n",
    "#     subfolder_path = os.path.join(working_dir, subfolder)\n",
    "#     if os.path.isdir(subfolder_path):  # Only process directories\n",
    "#         size_in_bytes = get_folder_size(subfolder_path)\n",
    "#         size_in_mb = size_in_bytes / (1024 * 1024)  # Convert to MB\n",
    "#         print(f\"Size of '{subfolder}': {size_in_mb:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T09:23:42.651909Z",
     "iopub.status.busy": "2024-11-14T09:23:42.651510Z",
     "iopub.status.idle": "2024-11-14T09:23:44.139629Z",
     "shell.execute_reply": "2024-11-14T09:23:44.138660Z",
     "shell.execute_reply.started": "2024-11-14T09:23:42.651871Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/working_directory.zip has been deleted.\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "\n",
    "# # Define the path to the file\n",
    "# file_path = '/kaggle/working/working_directory.zip'\n",
    "\n",
    "# # Check if the file exists, then delete it\n",
    "# if os.path.exists(file_path):\n",
    "#     os.remove(file_path)\n",
    "#     print(f\"{file_path} has been deleted.\")\n",
    "# else:\n",
    "#     print(f\"{file_path} does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-03-05T16:54:07.699658Z",
     "iopub.status.busy": "2025-03-05T16:54:07.699300Z",
     "iopub.status.idle": "2025-03-05T16:54:12.862468Z",
     "shell.execute_reply": "2025-03-05T16:54:12.861621Z",
     "shell.execute_reply.started": "2025-03-05T16:54:07.699629Z"
    },
    "executionInfo": {
     "elapsed": 6155,
     "status": "ok",
     "timestamp": 1715501062200,
     "user": {
      "displayName": "Shree Krishna Jamakatel",
      "userId": "14328212804148880429"
     },
     "user_tz": -345
    },
    "id": "3Ag65tVIk5cd",
    "outputId": "6cec61e7-2153-4f93-d0fe-873ec1e0a73b",
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `disable_exllama` is deprecated and will be removed in version 4.37. Use `use_exllama` instead and specify the version with `exllama_config`.The value of `use_exllama` will be overwritten by `disable_exllama` passed in `GPTQConfig` or stored in your config file.\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--marcsun13--gemma-2-9b-it-GPTQ/snapshots/9c51e21691674594afc671d8edaeb93aa0cc9f7a/config.json\n",
      "Model config Gemma2Config {\n",
      "  \"_name_or_path\": \"marcsun13/gemma-2-9b-it-GPTQ\",\n",
      "  \"architectures\": [\n",
      "    \"Gemma2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attn_logit_softcapping\": 50.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"final_logit_softcapping\": 30.0,\n",
      "  \"head_dim\": 256,\n",
      "  \"hidden_act\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"gemma2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 42,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"quantization_config\": {\n",
      "    \"batch_size\": 1,\n",
      "    \"bits\": 4,\n",
      "    \"block_name_to_quantize\": null,\n",
      "    \"cache_block_outputs\": true,\n",
      "    \"damp_percent\": 0.1,\n",
      "    \"dataset\": \"c4\",\n",
      "    \"desc_act\": false,\n",
      "    \"exllama_config\": {\n",
      "      \"version\": 2\n",
      "    },\n",
      "    \"group_size\": 128,\n",
      "    \"max_input_length\": null,\n",
      "    \"model_seqlen\": null,\n",
      "    \"module_name_preceding_first_block\": null,\n",
      "    \"modules_in_block_to_quantize\": null,\n",
      "    \"pad_token_id\": null,\n",
      "    \"quant_method\": \"gptq\",\n",
      "    \"sym\": true,\n",
      "    \"tokenizer\": null,\n",
      "    \"true_sequential\": true,\n",
      "    \"use_cuda_fp16\": false,\n",
      "    \"use_exllama\": true\n",
      "  },\n",
      "  \"query_pre_attn_scalar\": 224,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"sliding_window_size\": 4096,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 256000\n",
      "}\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/quantizers/auto.py:206: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.However, loading attributes (e.g. ['backend', 'use_cuda_fp16', 'use_exllama', 'max_input_length', 'exllama_config', 'disable_exllama']) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\n",
      "  warnings.warn(warning_msg)\n",
      "Loading the model in `torch.float16`. To overwrite it, set `torch_dtype` manually.\n",
      "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--marcsun13--gemma-2-9b-it-GPTQ/snapshots/9c51e21691674594afc671d8edaeb93aa0cc9f7a/model.safetensors.index.json\n",
      "Instantiating Gemma2ForCausalLM model under default dtype torch.float16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py:1536: UserWarning: Current model requires 4330475776 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "650eb2cf5ed14dffb1bb517b89f25d59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at marcsun13/gemma-2-9b-it-GPTQ were not used when initializing Gemma2ForCausalLM: {'model.layers.35.self_attn.k_proj.bias', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.38.mlp.up_proj.bias', 'model.layers.41.self_attn.k_proj.bias', 'model.layers.33.mlp.gate_proj.bias', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.20.mlp.gate_proj.bias', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.31.mlp.up_proj.bias', 'model.layers.7.mlp.gate_proj.bias', 'model.layers.41.mlp.gate_proj.bias', 'model.layers.33.self_attn.o_proj.bias', 'model.layers.24.mlp.up_proj.bias', 'model.layers.14.self_attn.o_proj.bias', 'model.layers.11.mlp.up_proj.bias', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.25.mlp.down_proj.bias', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.40.mlp.up_proj.bias', 'model.layers.21.mlp.gate_proj.bias', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.38.mlp.down_proj.bias', 'model.layers.34.mlp.gate_proj.bias', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.3.mlp.gate_proj.bias', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.36.self_attn.v_proj.bias', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.39.self_attn.k_proj.bias', 'model.layers.32.self_attn.v_proj.bias', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.40.self_attn.o_proj.bias', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.0.mlp.gate_proj.bias', 'model.layers.12.mlp.down_proj.bias', 'model.layers.29.mlp.up_proj.bias', 'model.layers.33.self_attn.k_proj.bias', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.22.mlp.down_proj.bias', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.32.self_attn.k_proj.bias', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.28.mlp.gate_proj.bias', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.37.mlp.down_proj.bias', 'model.layers.7.mlp.up_proj.bias', 'model.layers.38.self_attn.k_proj.bias', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.33.mlp.down_proj.bias', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.9.mlp.up_proj.bias', 'model.layers.27.mlp.gate_proj.bias', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.34.self_attn.k_proj.bias', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.26.mlp.gate_proj.bias', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.40.self_attn.v_proj.bias', 'model.layers.26.mlp.up_proj.bias', 'model.layers.9.mlp.gate_proj.bias', 'model.layers.39.self_attn.o_proj.bias', 'model.layers.10.mlp.up_proj.bias', 'model.layers.25.mlp.up_proj.bias', 'model.layers.25.self_attn.o_proj.bias', 'model.layers.34.self_attn.q_proj.bias', 'model.layers.32.self_attn.o_proj.bias', 'model.layers.8.mlp.down_proj.bias', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.5.mlp.up_proj.bias', 'model.layers.19.mlp.down_proj.bias', 'model.layers.8.mlp.up_proj.bias', 'model.layers.30.self_attn.o_proj.bias', 'model.layers.5.mlp.gate_proj.bias', 'model.layers.0.mlp.up_proj.bias', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.6.mlp.up_proj.bias', 'model.layers.4.mlp.down_proj.bias', 'model.layers.27.mlp.up_proj.bias', 'model.layers.18.mlp.down_proj.bias', 'model.layers.24.self_attn.o_proj.bias', 'model.layers.17.mlp.gate_proj.bias', 'model.layers.2.mlp.up_proj.bias', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.4.mlp.gate_proj.bias', 'model.layers.8.mlp.gate_proj.bias', 'model.layers.21.mlp.down_proj.bias', 'model.layers.15.mlp.up_proj.bias', 'model.layers.2.mlp.down_proj.bias', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.16.mlp.gate_proj.bias', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.22.mlp.gate_proj.bias', 'model.layers.24.mlp.down_proj.bias', 'model.layers.29.mlp.gate_proj.bias', 'model.layers.28.self_attn.o_proj.bias', 'model.layers.33.self_attn.q_proj.bias', 'model.layers.32.mlp.up_proj.bias', 'model.layers.19.mlp.up_proj.bias', 'model.layers.27.mlp.down_proj.bias', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.32.mlp.gate_proj.bias', 'model.layers.37.self_attn.o_proj.bias', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.36.mlp.down_proj.bias', 'model.layers.12.mlp.gate_proj.bias', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.38.self_attn.q_proj.bias', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.35.self_attn.v_proj.bias', 'model.layers.12.mlp.up_proj.bias', 'model.layers.34.mlp.down_proj.bias', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.1.mlp.up_proj.bias', 'model.layers.40.mlp.gate_proj.bias', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.33.mlp.up_proj.bias', 'model.layers.16.mlp.up_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.41.self_attn.v_proj.bias', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.17.mlp.down_proj.bias', 'model.layers.36.mlp.up_proj.bias', 'model.layers.31.mlp.gate_proj.bias', 'model.layers.3.mlp.down_proj.bias', 'model.layers.37.mlp.gate_proj.bias', 'model.layers.21.mlp.up_proj.bias', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.14.mlp.up_proj.bias', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.22.self_attn.o_proj.bias', 'model.layers.34.mlp.up_proj.bias', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.30.mlp.gate_proj.bias', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.30.mlp.down_proj.bias', 'model.layers.36.self_attn.k_proj.bias', 'model.layers.37.self_attn.q_proj.bias', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.14.mlp.down_proj.bias', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.34.self_attn.o_proj.bias', 'model.layers.39.self_attn.q_proj.bias', 'model.layers.17.self_attn.o_proj.bias', 'model.layers.32.mlp.down_proj.bias', 'model.layers.40.self_attn.k_proj.bias', 'model.layers.39.mlp.gate_proj.bias', 'model.layers.32.self_attn.q_proj.bias', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.13.mlp.gate_proj.bias', 'model.layers.13.mlp.up_proj.bias', 'model.layers.25.mlp.gate_proj.bias', 'model.layers.20.mlp.up_proj.bias', 'model.layers.15.mlp.down_proj.bias', 'model.layers.26.self_attn.o_proj.bias', 'model.layers.39.mlp.down_proj.bias', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.10.mlp.gate_proj.bias', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.34.self_attn.v_proj.bias', 'model.layers.38.self_attn.o_proj.bias', 'model.layers.9.mlp.down_proj.bias', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.29.self_attn.o_proj.bias', 'model.layers.3.mlp.up_proj.bias', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.35.mlp.gate_proj.bias', 'model.layers.16.mlp.down_proj.bias', 'model.layers.37.self_attn.v_proj.bias', 'model.layers.39.self_attn.v_proj.bias', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.38.self_attn.v_proj.bias', 'model.layers.36.mlp.gate_proj.bias', 'model.layers.28.mlp.up_proj.bias', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.11.mlp.down_proj.bias', 'model.layers.1.mlp.down_proj.bias', 'model.layers.31.self_attn.o_proj.bias', 'model.layers.41.self_attn.q_proj.bias', 'model.layers.41.mlp.down_proj.bias', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.0.mlp.down_proj.bias', 'model.layers.18.mlp.gate_proj.bias', 'model.layers.9.self_attn.o_proj.bias', 'model.layers.39.mlp.up_proj.bias', 'model.layers.30.mlp.up_proj.bias', 'model.layers.17.mlp.up_proj.bias', 'model.layers.13.mlp.down_proj.bias', 'model.layers.11.mlp.gate_proj.bias', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.10.mlp.down_proj.bias', 'model.layers.7.mlp.down_proj.bias', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.23.mlp.up_proj.bias', 'model.layers.2.mlp.gate_proj.bias', 'model.layers.6.mlp.down_proj.bias', 'model.layers.41.self_attn.o_proj.bias', 'model.layers.18.mlp.up_proj.bias', 'model.layers.5.mlp.down_proj.bias', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.20.mlp.down_proj.bias', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.27.self_attn.o_proj.bias', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.15.mlp.gate_proj.bias', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.40.self_attn.q_proj.bias', 'model.layers.4.mlp.up_proj.bias', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.22.mlp.up_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.23.self_attn.o_proj.bias', 'model.layers.23.mlp.gate_proj.bias', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.33.self_attn.v_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.40.mlp.down_proj.bias', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.6.mlp.gate_proj.bias', 'model.layers.36.self_attn.o_proj.bias', 'model.layers.26.mlp.down_proj.bias', 'model.layers.37.mlp.up_proj.bias', 'model.layers.28.mlp.down_proj.bias', 'model.layers.31.mlp.down_proj.bias', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.36.self_attn.q_proj.bias', 'model.layers.35.mlp.up_proj.bias', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.23.mlp.down_proj.bias', 'model.layers.24.mlp.gate_proj.bias', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.35.self_attn.o_proj.bias', 'model.layers.38.mlp.gate_proj.bias', 'model.layers.14.mlp.gate_proj.bias', 'model.layers.29.mlp.down_proj.bias', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.37.self_attn.k_proj.bias', 'model.layers.41.mlp.up_proj.bias', 'model.layers.1.mlp.gate_proj.bias', 'model.layers.35.self_attn.q_proj.bias', 'model.layers.19.mlp.gate_proj.bias', 'model.layers.35.mlp.down_proj.bias', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.23.self_attn.k_proj.bias'}\n",
      "- This IS expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Gemma2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of Gemma2ForCausalLM were initialized from the model checkpoint at marcsun13/gemma-2-9b-it-GPTQ.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Gemma2ForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--marcsun13--gemma-2-9b-it-GPTQ/snapshots/9c51e21691674594afc671d8edaeb93aa0cc9f7a/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "quantization_config_loading = GPTQConfig(bits=4, disable_exllama=True)\n",
    "\n",
    "# disable_exllama=True\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(output_dir_new,quantization_config=quantization_config_loading, device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-03-05T16:54:23.740015Z",
     "iopub.status.busy": "2025-03-05T16:54:23.739697Z",
     "iopub.status.idle": "2025-03-05T16:54:23.746621Z",
     "shell.execute_reply": "2025-03-05T16:54:23.745665Z",
     "shell.execute_reply.started": "2025-03-05T16:54:23.739987Z"
    },
    "executionInfo": {
     "elapsed": 438,
     "status": "ok",
     "timestamp": 1715501071080,
     "user": {
      "displayName": "Shree Krishna Jamakatel",
      "userId": "14328212804148880429"
     },
     "user_tz": -345
    },
    "id": "62hn97okliHx",
    "outputId": "68d0ef09-9dc8-4a07-b73c-fa08fb62f60d",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '<|system|>\\n<eos>\\n<|user|>\\n<DOCUMENT>Primary injection test sets\\nODEN A and ODEN AT\\nCreated to test low-voltage and primary breakers, ODEN \\nis designed to generate extraordinarily high power with-\\nout sacrificing portability. Outputs can range up to many \\nthousand amperes, and thanks to a highly sophisticated \\nmeasurement section, ODEN has a very broad range of  \\napplications.\\nINGVAR\\nThis powerful test system is designed for primary injection \\ntesting of  protective relay equipment and circuit breakers. \\nThe system consists of  a control unit and a current unit. \\n</DOCUMENT>\\n<DOCUMENT>PF tests for shielded or \\nsheathed cables and accessories are diagnostic testing methods for field testing of \\nservice aged cable installations. For lossless insulation,  the cable capacitance (C ) \\nper unit length can be calculated using the following formula: \\n𝐶 = 2𝜋𝑘𝑒0 ln(𝑑𝑖\\n𝑑𝑐\\n) \\nwhere \\nk is the insulation dielectric constant \\ne0 is the permittivity (capacitance) of air \\ndi is the diameter over the insulation \\ndc is the conductor diameter \\n \\nFor power cables with conventional insulating materials, the cable conductance (G ) \\nper unit length can be calculated using the following formula: \\n𝐺 = 2𝜋𝑓𝐶tan𝛿 \\n \\nThe quantity tan δ presents the losses in t he insulation when exposed to an electric \\nfield and is known as DF or the insulating material loss angle. Table 1 gives typical \\nvalues of dielectric constant k and tan δ.</DOCUMENT>\\nWhat formula is used to calculate the capacitance per unit length of a cable with lossless insulation?<eos>\\n<|assistant|>\\nassistant: To answer the question, we need to identify the formula used to calculate the capacitance per unit length of a cable with lossless insulation. The context provides a specific formula for this scenario.\\n\\n##begin_quote##\\nFor lossless insulation, the cable capacitance (C ) per unit length can be calculated using the following formula: \\n𝐶 = 2𝜋𝑘𝑒0 ln(𝑑𝑖\\n𝑑𝑐\\n) \\nwhere \\nk is the insulation dielectric constant \\ne0 is the permittivity (capacitance) of air \\ndi is the diameter over the insulation \\ndc is the conductor diameter\\n##end_quote##\\n\\nThe formula provided in the context directly answers the question. It specifies the capacitance per unit length for a cable with lossless insulation, which is what the question asks for.\\n\\n<ANSWER>: 𝐶 = 2𝜋𝑘𝑒0 ln(𝑑𝑖𝑑𝑐)<eos>\\n'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"test\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FGIN3wyx1ZZ5"
   },
   "source": [
    "## Use Query only for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-03-05T16:54:26.904034Z",
     "iopub.status.busy": "2025-03-05T16:54:26.903723Z",
     "iopub.status.idle": "2025-03-05T16:55:57.657760Z",
     "shell.execute_reply": "2025-03-05T16:55:57.655914Z",
     "shell.execute_reply.started": "2025-03-05T16:54:26.904010Z"
    },
    "executionInfo": {
     "elapsed": 12184,
     "status": "ok",
     "timestamp": 1715501310870,
     "user": {
      "displayName": "Shree Krishna Jamakatel",
      "userId": "14328212804148880429"
     },
     "user_tz": -345
    },
    "id": "U0WMyPeMlVh-",
    "outputId": "04dc4fb3-cfbc-48d0-e8ba-62cf198aff7e",
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "ename": "BackendCompilerFailed",
     "evalue": "backend='inductor' raised:\nRuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\u001b[0m in \u001b[0;36m_call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m   1445\u001b[0m                 \u001b[0mcompiler_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWrapperBackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompiler_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1446\u001b[0;31m             \u001b[0mcompiled_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiler_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexample_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1447\u001b[0m             \u001b[0m_step_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"done compiler function {name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0mcompiled_gm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiler_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0mcompiled_gm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiler_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, model_, inputs_)\u001b[0m\n\u001b[1;32m   2233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcompile_fx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_patches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\u001b[0m in \u001b[0;36mcompile_fx\u001b[0;34m(model_, example_inputs_, inner_compile, config_patches, decompositions)\u001b[0m\n\u001b[1;32m   1252\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_patches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1253\u001b[0;31m                 return compile_fx(\n\u001b[0m\u001b[1;32m   1254\u001b[0m                     \u001b[0mmodel_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\u001b[0m in \u001b[0;36mcompile_fx\u001b[0;34m(model_, example_inputs_, inner_compile, config_patches, decompositions)\u001b[0m\n\u001b[1;32m   1520\u001b[0m         ):\n\u001b[0;32m-> 1521\u001b[0;31m             return aot_autograd(\n\u001b[0m\u001b[1;32m   1522\u001b[0m                 \u001b[0mfw_compiler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfw_compiler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0menable_aot_logging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                 \u001b[0mcg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maot_module_simplified\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m                 \u001b[0mcounters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"aot_autograd\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ok\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\u001b[0m in \u001b[0;36maot_module_simplified\u001b[0;34m(mod, args, fw_compiler, bw_compiler, partition_fn, decompositions, keep_inference_input_mutations, inference_compiler, cudagraphs)\u001b[0m\n\u001b[1;32m   1070\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m         \u001b[0mcompiled_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatch_and_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\u001b[0m in \u001b[0;36mdispatch_and_compile\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcompiled_autograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             compiled_fn, _ = create_aot_dispatcher_function(\n\u001b[0m\u001b[1;32m   1057\u001b[0m                 \u001b[0mfunctional_call\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\u001b[0m in \u001b[0;36mcreate_aot_dispatcher_function\u001b[0;34m(flat_fn, fake_flat_args, aot_config, fake_mode, shape_env)\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mdynamo_timed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"create_aot_dispatcher_function\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m         return _create_aot_dispatcher_function(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mflat_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_flat_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maot_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape_env\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\u001b[0m in \u001b[0;36m_create_aot_dispatcher_function\u001b[0;34m(flat_fn, fake_flat_args, aot_config, fake_mode, shape_env)\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m         compiled_fn, fw_metadata = compiler_fn(\n\u001b[0m\u001b[1;32m    760\u001b[0m             \u001b[0mflat_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\u001b[0m in \u001b[0;36maot_dispatch_base\u001b[0;34m(flat_fn, flat_args, aot_config, fw_metadata)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTracingContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreport_output_strides\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfwd_output_strides\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mcompiled_fw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfw_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdated_flat_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\u001b[0m in \u001b[0;36mfw_compiler_base\u001b[0;34m(model, example_inputs, is_inference)\u001b[0m\n\u001b[1;32m   1349\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mdynamo_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamo_timed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"compile_fx.<locals>.fw_compiler_base\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_fw_compiler_base\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_inference\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\u001b[0m in \u001b[0;36m_fw_compiler_base\u001b[0;34m(model, example_inputs, is_inference)\u001b[0m\n\u001b[1;32m   1420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1421\u001b[0;31m             return inner_compile(\n\u001b[0m\u001b[1;32m   1422\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/contextlib.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recreate_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\u001b[0m in \u001b[0;36mcompile_fx_inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\u001b[0m in \u001b[0;36mdebug_wrapper\u001b[0;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;31m# with fake inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0minner_compiled_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiler_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\u001b[0m in \u001b[0;36m_compile_fx_inner\u001b[0;34m(gm, example_inputs, cudagraphs, static_input_idxs, is_backward, graph_id, cpp_wrapper, aot_mode, is_inference, boxed_forward_device_index, user_visible_outputs, layout_opt, extern_node_serializer)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m             compiled_graph = FxGraphCache.load(\n\u001b[0m\u001b[1;32m    662\u001b[0m                 \u001b[0mcodegen_and_compile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(compile_fx_fn, gm, example_inputs, fx_kwargs, inputs_to_check, local, remote)\u001b[0m\n\u001b[1;32m   1333\u001b[0m                 \u001b[0mcache_event_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m                 compiled_graph = compile_fx_fn(\n\u001b[0m\u001b[1;32m   1335\u001b[0m                     \u001b[0mgm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_to_check\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfx_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\u001b[0m in \u001b[0;36mcodegen_and_compile\u001b[0;34m(gm, example_inputs, inputs_to_check, fx_kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \"\"\"\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mcompiled_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfx_codegen_and_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfx_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompiled_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\u001b[0m in \u001b[0;36mfx_codegen_and_compile\u001b[0;34m(gm, example_inputs, cudagraphs, static_input_idxs, is_backward, graph_id, cpp_wrapper, aot_mode, is_inference, user_visible_outputs, layout_opt, extern_node_serializer)\u001b[0m\n\u001b[1;32m    877\u001b[0m                 \u001b[0m_check_triton_bf16_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m                 \u001b[0mcompiled_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_to_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    879\u001b[0m                 \u001b[0mnum_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes_num_elem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_runtimes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\u001b[0m in \u001b[0;36mcompile_to_fn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1912\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1913\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_to_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\u001b[0m in \u001b[0;36mcompile_to_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1838\u001b[0m         ):\n\u001b[0;32m-> 1839\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compile_to_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\u001b[0m in \u001b[0;36m_compile_to_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1844\u001b[0m         code, linemap = (\n\u001b[0;32m-> 1845\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodegen_with_cpp_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpp_wrapper\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodegen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1846\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\u001b[0m in \u001b[0;36mcodegen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1780\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mScheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moperations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1781\u001b[0m         \u001b[0mV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_orig_fx_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_gm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, nodes)\u001b[0m\n\u001b[1;32m   1730\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdynamo_timed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Scheduler.__init__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1731\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\u001b[0m in \u001b[0;36m_init\u001b[0;34m(self, nodes)\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_scheduler_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_zero_dim_cpu_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_scheduler_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_zero_dim_cpu_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\u001b[0m in \u001b[0;36mcreate_scheduler_node\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m   1855\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mComputedBuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTemplateBuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1856\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mSchedulerNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1857\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExternKernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, scheduler, node)\u001b[0m\n\u001b[1;32m    832\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_from_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_attrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\u001b[0m in \u001b[0;36m_compute_attrs\u001b[0;34m(self, extra_indexing_constraints, recompute_sizes_body_func)\u001b[0m\n\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m         \u001b[0mgroup_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\u001b[0m in \u001b[0;36mget_backend\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m   3359\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3360\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3361\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\u001b[0m in \u001b[0;36mcreate_backend\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m   3347\u001b[0m             ):\n\u001b[0;32m-> 3348\u001b[0;31m                 raise RuntimeError(\n\u001b[0m\u001b[1;32m   3349\u001b[0m                     \u001b[0;34mf\"Found {device_props.name} which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability {device_props.major}.{device_props.minor}\"\u001b[0m  \u001b[0;31m# noqa: B950\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mBackendCompilerFailed\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-49f29a0b04a3>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m outputs = model.generate(\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2222\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2223\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2224\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2225\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3212\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3213\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3214\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3216\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m                 \u001b[0;31m# Restore the dynamic layer stack depth if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, frame, cache_entry, frame_state)\u001b[0m\n\u001b[1;32m   1267\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcompile_lock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_disable_current_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m             \u001b[0;31m# skip=1: skip this frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1269\u001b[0;31m             return self._torchdynamo_orig_callable(\n\u001b[0m\u001b[1;32m   1270\u001b[0m                 \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_entry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m         return _compile(\n\u001b[0m\u001b[1;32m    527\u001b[0m             \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_globals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_entry, cache_size, frame, frame_state, compile_id, skip)\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0mguarded_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m             \u001b[0mguarded_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompile_inner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mguarded_code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36mcompile_inner\u001b[0;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdynamo_timed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_compile.compile_inner\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphase_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"entire_frame_compile\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mCompileTimeInstructionCounter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 666\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_compile_inner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcompile_time_strobelight_meta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"compile_inner\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\u001b[0m in \u001b[0;36mwrapper_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mStrobelightCompileTimeProfiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             return StrobelightCompileTimeProfiler.profile_compile_time(\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m_compile_inner\u001b[0;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0mCompileContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattempt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattempt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                 \u001b[0mout_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_code_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRestartAnalysis\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\u001b[0m in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m   1320\u001b[0m     \u001b[0mpropagate_line_nums\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstructions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m     \u001b[0mtransformations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstructions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mclean_and_assemble_instructions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstructions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m             )\n\u001b[1;32m    218\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                 \u001b[0mcleanup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracing_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_current_tx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m                 \u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnspecializeRestartAnalysis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0mspeculation_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2796\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2798\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmatch_nested_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    981\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_tx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 983\u001b[0;31m                 \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    984\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mBackendCompilerFailed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopcode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObservedException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mRETURN_VALUE\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   2985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2986\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mRETURN_VALUE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2987\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2989\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mRETURN_CONST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36m_return\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   2970\u001b[0m         )\n\u001b[1;32m   2971\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s triggered compile\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2972\u001b[0;31m         self.output.compile_subgraph(\n\u001b[0m\u001b[1;32m   2973\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2974\u001b[0m             reason=GraphCompileReason(\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\u001b[0m in \u001b[0;36mcompile_subgraph\u001b[0;34m(self, tx, partial_convert, reason)\u001b[0m\n\u001b[1;32m   1140\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcount_calls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpass2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_outputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 output.extend(\n\u001b[0;32m-> 1142\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_and_call_fx_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpass2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_output_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1143\u001b[0m                 )\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\u001b[0m in \u001b[0;36mcompile_and_call_fx_graph\u001b[0;34m(self, tx, rv, root)\u001b[0m\n\u001b[1;32m   1367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore_global_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1369\u001b[0;31m                 \u001b[0mcompiled_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_user_compiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy_graph_module\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_LazyGraphModule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\u001b[0m in \u001b[0;36mcall_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m   1414\u001b[0m             \u001b[0;34m\"OutputGraph.call_user_compiler\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphase_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"backend_compile\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1415\u001b[0m         ):\n\u001b[0;32m-> 1416\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_user_compiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_user_compiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphModule\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mCompiledFn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\u001b[0m in \u001b[0;36m_call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m   1463\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1465\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mBackendCompilerFailed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiler_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m         signpost_event(\n",
      "\u001b[0;31mBackendCompilerFailed\u001b[0m: backend='inductor' raised:\nRuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "\n",
    "messages = [\n",
    "\n",
    "    {\n",
    "\n",
    "        \"role\": \"system\",\n",
    "\n",
    "        \"content\": \"You are a RAFT model you give answer based on context. You also give reason for why this is the answer.\",\n",
    "\n",
    "    },\n",
    "\n",
    "    {\"role\": \"user\", \"content\": \"What did Paul Graham believe was the future of web development?\"},\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# prepare the messages for the model\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(messages, truncation=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "\n",
    "\n",
    "# inference\n",
    "\n",
    "outputs = model.generate(\n",
    "\n",
    "        input_ids=input_ids,\n",
    "\n",
    "        max_new_tokens=500,\n",
    "\n",
    "        do_sample=True,\n",
    "\n",
    "        temperature=0.7,\n",
    "\n",
    "        top_k=50,\n",
    "\n",
    "        top_p=0.95\n",
    "\n",
    ")\n",
    "\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Query along with retreived context for inference. \n",
    "\n",
    "* This is taken from the test set we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T16:56:23.452993Z",
     "iopub.status.busy": "2025-03-05T16:56:23.452661Z",
     "iopub.status.idle": "2025-03-05T16:56:23.458233Z",
     "shell.execute_reply": "2025-03-05T16:56:23.457183Z",
     "shell.execute_reply.started": "2025-03-05T16:56:23.452967Z"
    },
    "id": "BRVktB7z2Gty",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "context_and_query = \"\"\"\n",
    "\n",
    "\\n<DOCUMENT>And it wasn't, so I would.\\n\\nIn the summer of 2012 my mother had a stroke, and the cause turned out to be a blood clot caused by colon cancer. The stroke destroyed her balance, and she was put in a nursing home, but she really wanted to get out of it and back to her house, and my sister and I were determined to help her do it. I used to fly up to Oregon to visit her regularly, and I had a lot of time to think on those flights. </DOCUMENT>\\n<DOCUMENT>I had no idea. </DOCUMENT>\\n<DOCUMENT>Around this time, in the spring of 2000, I had an idea. It was clear from our experience with Viaweb that web apps were the future. Why not build a web app for making web apps? Why not let people edit code on our server through the browser, and then host the resulting applications for them? [9] You could run all sorts of services on the servers that these applications could use just by making an API call: making and receiving phone calls, manipulating images, taking credit card payments, etc.\\n\\nI got so excited about this idea that I couldn't think about anything else. </DOCUMENT>\\n\n",
    "\n",
    "What did Paul Graham believe was the future of web development?\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-05T16:56:25.241504Z",
     "iopub.status.busy": "2025-03-05T16:56:25.241167Z",
     "iopub.status.idle": "2025-03-05T16:56:25.246425Z",
     "shell.execute_reply": "2025-03-05T16:56:25.245647Z",
     "shell.execute_reply.started": "2025-03-05T16:56:25.241476Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "context_and_query = \"\"\"\n",
    "\n",
    "\\n<DOCUMENT> Emma is a creative and driven designer known for her innovative approach to user experience. With a keen eye for detail and a background in psychology, she combines aesthetics with usability, crafting interfaces that feel intuitive and engaging. Colleagues admire her collaborative spirit and problem-solving skills, as she thrives in fast-paced environments and enjoys tackling complex design challenges. Outside of work, Emma is passionate about photography and often finds inspiration in nature, incorporating earthy tones and organic shapes into her projects. Her dedication to continuous learning and user-centered design has earned her a respected place in her field. </DOCUMENT>\\n<DOCUMENT>I had no idea. </DOCUMENT>\\n<DOCUMENT>James is an insightful engineer with a knack for simplifying complex systems. With a foundation in mechanical engineering and a passion for sustainable technology, he’s developed solutions that enhance efficiency and reduce environmental impact. Known for his meticulous approach, James excels at troubleshooting and enjoys mentoring younger engineers, helping them navigate intricate technical challenges. Outside of work, he’s an avid cyclist and enjoys restoring vintage bikes, blending his technical skills with his love for craftsmanship. James’s commitment to innovation and teamwork makes him a valued member of his team and a respected figure in his industry.</DOCUMENT>\\n\n",
    "\n",
    "Why James is a valued member?\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-03-05T16:56:27.129385Z",
     "iopub.status.busy": "2025-03-05T16:56:27.129025Z",
     "iopub.status.idle": "2025-03-05T16:58:18.515101Z",
     "shell.execute_reply": "2025-03-05T16:58:18.513380Z",
     "shell.execute_reply.started": "2025-03-05T16:56:27.129359Z"
    },
    "executionInfo": {
     "elapsed": 9815,
     "status": "ok",
     "timestamp": 1715501570041,
     "user": {
      "displayName": "Shree Krishna Jamakatel",
      "userId": "14328212804148880429"
     },
     "user_tz": -345
    },
    "id": "B5hQ1bRI29tf",
    "outputId": "3af55738-156d-4d30-b8ab-3ba5d803a7c0",
    "trusted": true
   },
   "outputs": [
    {
     "ename": "BackendCompilerFailed",
     "evalue": "backend='inductor' raised:\nRuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\u001b[0m in \u001b[0;36m_call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m   1445\u001b[0m                 \u001b[0mcompiler_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWrapperBackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompiler_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1446\u001b[0;31m             \u001b[0mcompiled_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiler_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexample_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1447\u001b[0m             \u001b[0m_step_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"done compiler function {name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0mcompiled_gm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiler_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0mcompiled_gm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiler_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, model_, inputs_)\u001b[0m\n\u001b[1;32m   2233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcompile_fx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_patches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\u001b[0m in \u001b[0;36mcompile_fx\u001b[0;34m(model_, example_inputs_, inner_compile, config_patches, decompositions)\u001b[0m\n\u001b[1;32m   1252\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_patches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1253\u001b[0;31m                 return compile_fx(\n\u001b[0m\u001b[1;32m   1254\u001b[0m                     \u001b[0mmodel_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\u001b[0m in \u001b[0;36mcompile_fx\u001b[0;34m(model_, example_inputs_, inner_compile, config_patches, decompositions)\u001b[0m\n\u001b[1;32m   1520\u001b[0m         ):\n\u001b[0;32m-> 1521\u001b[0;31m             return aot_autograd(\n\u001b[0m\u001b[1;32m   1522\u001b[0m                 \u001b[0mfw_compiler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfw_compiler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0menable_aot_logging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                 \u001b[0mcg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maot_module_simplified\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m                 \u001b[0mcounters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"aot_autograd\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ok\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\u001b[0m in \u001b[0;36maot_module_simplified\u001b[0;34m(mod, args, fw_compiler, bw_compiler, partition_fn, decompositions, keep_inference_input_mutations, inference_compiler, cudagraphs)\u001b[0m\n\u001b[1;32m   1070\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m         \u001b[0mcompiled_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatch_and_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\u001b[0m in \u001b[0;36mdispatch_and_compile\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcompiled_autograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             compiled_fn, _ = create_aot_dispatcher_function(\n\u001b[0m\u001b[1;32m   1057\u001b[0m                 \u001b[0mfunctional_call\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\u001b[0m in \u001b[0;36mcreate_aot_dispatcher_function\u001b[0;34m(flat_fn, fake_flat_args, aot_config, fake_mode, shape_env)\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mdynamo_timed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"create_aot_dispatcher_function\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m         return _create_aot_dispatcher_function(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mflat_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_flat_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maot_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape_env\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\u001b[0m in \u001b[0;36m_create_aot_dispatcher_function\u001b[0;34m(flat_fn, fake_flat_args, aot_config, fake_mode, shape_env)\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m         compiled_fn, fw_metadata = compiler_fn(\n\u001b[0m\u001b[1;32m    760\u001b[0m             \u001b[0mflat_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\u001b[0m in \u001b[0;36maot_dispatch_base\u001b[0;34m(flat_fn, flat_args, aot_config, fw_metadata)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTracingContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreport_output_strides\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfwd_output_strides\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mcompiled_fw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfw_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdated_flat_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\u001b[0m in \u001b[0;36mfw_compiler_base\u001b[0;34m(model, example_inputs, is_inference)\u001b[0m\n\u001b[1;32m   1349\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mdynamo_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamo_timed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"compile_fx.<locals>.fw_compiler_base\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_fw_compiler_base\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_inference\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\u001b[0m in \u001b[0;36m_fw_compiler_base\u001b[0;34m(model, example_inputs, is_inference)\u001b[0m\n\u001b[1;32m   1420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1421\u001b[0;31m             return inner_compile(\n\u001b[0m\u001b[1;32m   1422\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/contextlib.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recreate_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\u001b[0m in \u001b[0;36mcompile_fx_inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\u001b[0m in \u001b[0;36mdebug_wrapper\u001b[0;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;31m# with fake inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0minner_compiled_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiler_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\u001b[0m in \u001b[0;36m_compile_fx_inner\u001b[0;34m(gm, example_inputs, cudagraphs, static_input_idxs, is_backward, graph_id, cpp_wrapper, aot_mode, is_inference, boxed_forward_device_index, user_visible_outputs, layout_opt, extern_node_serializer)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m             compiled_graph = FxGraphCache.load(\n\u001b[0m\u001b[1;32m    662\u001b[0m                 \u001b[0mcodegen_and_compile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(compile_fx_fn, gm, example_inputs, fx_kwargs, inputs_to_check, local, remote)\u001b[0m\n\u001b[1;32m   1333\u001b[0m                 \u001b[0mcache_event_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m                 compiled_graph = compile_fx_fn(\n\u001b[0m\u001b[1;32m   1335\u001b[0m                     \u001b[0mgm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_to_check\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfx_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\u001b[0m in \u001b[0;36mcodegen_and_compile\u001b[0;34m(gm, example_inputs, inputs_to_check, fx_kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \"\"\"\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mcompiled_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfx_codegen_and_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfx_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompiled_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\u001b[0m in \u001b[0;36mfx_codegen_and_compile\u001b[0;34m(gm, example_inputs, cudagraphs, static_input_idxs, is_backward, graph_id, cpp_wrapper, aot_mode, is_inference, user_visible_outputs, layout_opt, extern_node_serializer)\u001b[0m\n\u001b[1;32m    877\u001b[0m                 \u001b[0m_check_triton_bf16_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m                 \u001b[0mcompiled_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_to_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    879\u001b[0m                 \u001b[0mnum_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes_num_elem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_runtimes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\u001b[0m in \u001b[0;36mcompile_to_fn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1912\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1913\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_to_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\u001b[0m in \u001b[0;36mcompile_to_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1838\u001b[0m         ):\n\u001b[0;32m-> 1839\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compile_to_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\u001b[0m in \u001b[0;36m_compile_to_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1844\u001b[0m         code, linemap = (\n\u001b[0;32m-> 1845\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodegen_with_cpp_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpp_wrapper\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodegen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1846\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\u001b[0m in \u001b[0;36mcodegen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1780\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mScheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moperations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1781\u001b[0m         \u001b[0mV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_orig_fx_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_gm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, nodes)\u001b[0m\n\u001b[1;32m   1730\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdynamo_timed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Scheduler.__init__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1731\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\u001b[0m in \u001b[0;36m_init\u001b[0;34m(self, nodes)\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_scheduler_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_zero_dim_cpu_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_scheduler_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_zero_dim_cpu_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\u001b[0m in \u001b[0;36mcreate_scheduler_node\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m   1855\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mComputedBuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTemplateBuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1856\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mSchedulerNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1857\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExternKernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, scheduler, node)\u001b[0m\n\u001b[1;32m    832\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_from_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_attrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\u001b[0m in \u001b[0;36m_compute_attrs\u001b[0;34m(self, extra_indexing_constraints, recompute_sizes_body_func)\u001b[0m\n\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m         \u001b[0mgroup_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\u001b[0m in \u001b[0;36mget_backend\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m   3359\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3360\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3361\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\u001b[0m in \u001b[0;36mcreate_backend\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m   3347\u001b[0m             ):\n\u001b[0;32m-> 3348\u001b[0;31m                 raise RuntimeError(\n\u001b[0m\u001b[1;32m   3349\u001b[0m                     \u001b[0;34mf\"Found {device_props.name} which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability {device_props.major}.{device_props.minor}\"\u001b[0m  \u001b[0;31m# noqa: B950\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mBackendCompilerFailed\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-6bf33f8ca3f1>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m outputs = model.generate(\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2222\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2223\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2224\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2225\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3212\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3213\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3214\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3216\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m                 \u001b[0;31m# Restore the dynamic layer stack depth if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, frame, cache_entry, frame_state)\u001b[0m\n\u001b[1;32m   1267\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcompile_lock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_disable_current_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m             \u001b[0;31m# skip=1: skip this frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1269\u001b[0;31m             return self._torchdynamo_orig_callable(\n\u001b[0m\u001b[1;32m   1270\u001b[0m                 \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_entry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m         return _compile(\n\u001b[0m\u001b[1;32m    527\u001b[0m             \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_globals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_entry, cache_size, frame, frame_state, compile_id, skip)\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0mguarded_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m             \u001b[0mguarded_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompile_inner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mguarded_code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36mcompile_inner\u001b[0;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdynamo_timed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_compile.compile_inner\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphase_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"entire_frame_compile\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mCompileTimeInstructionCounter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 666\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_compile_inner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcompile_time_strobelight_meta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"compile_inner\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\u001b[0m in \u001b[0;36mwrapper_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mStrobelightCompileTimeProfiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             return StrobelightCompileTimeProfiler.profile_compile_time(\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m_compile_inner\u001b[0;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0mCompileContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattempt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattempt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                 \u001b[0mout_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_code_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRestartAnalysis\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\u001b[0m in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m   1320\u001b[0m     \u001b[0mpropagate_line_nums\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstructions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m     \u001b[0mtransformations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstructions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mclean_and_assemble_instructions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstructions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m             )\n\u001b[1;32m    218\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                 \u001b[0mcleanup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracing_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_current_tx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m                 \u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnspecializeRestartAnalysis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0mspeculation_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2796\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2798\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmatch_nested_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    981\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_tx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 983\u001b[0;31m                 \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    984\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mBackendCompilerFailed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopcode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObservedException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mRETURN_VALUE\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   2985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2986\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mRETURN_VALUE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2987\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2989\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mRETURN_CONST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36m_return\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   2970\u001b[0m         )\n\u001b[1;32m   2971\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s triggered compile\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2972\u001b[0;31m         self.output.compile_subgraph(\n\u001b[0m\u001b[1;32m   2973\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2974\u001b[0m             reason=GraphCompileReason(\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\u001b[0m in \u001b[0;36mcompile_subgraph\u001b[0;34m(self, tx, partial_convert, reason)\u001b[0m\n\u001b[1;32m   1140\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcount_calls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpass2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_outputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 output.extend(\n\u001b[0;32m-> 1142\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_and_call_fx_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpass2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_output_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1143\u001b[0m                 )\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\u001b[0m in \u001b[0;36mcompile_and_call_fx_graph\u001b[0;34m(self, tx, rv, root)\u001b[0m\n\u001b[1;32m   1367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore_global_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1369\u001b[0;31m                 \u001b[0mcompiled_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_user_compiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy_graph_module\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_LazyGraphModule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\u001b[0m in \u001b[0;36mcall_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m   1414\u001b[0m             \u001b[0;34m\"OutputGraph.call_user_compiler\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphase_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"backend_compile\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1415\u001b[0m         ):\n\u001b[0;32m-> 1416\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_user_compiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_user_compiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphModule\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mCompiledFn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\u001b[0m in \u001b[0;36m_call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m   1463\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1465\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mBackendCompilerFailed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiler_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m         signpost_event(\n",
      "\u001b[0;31mBackendCompilerFailed\u001b[0m: backend='inductor' raised:\nRuntimeError: Found Tesla P100-PCIE-16GB which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.0\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "\n",
    "messages = [\n",
    "\n",
    "    {\n",
    "\n",
    "        \"role\": \"system\",\n",
    "\n",
    "        \"content\": \"You are a RAFT model you give answer based on context. You also give reason for why this is the answer.\",\n",
    "\n",
    "    },\n",
    "\n",
    "    {\"role\": \"user\", \"content\": context_and_query},\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# prepare the messages for the model\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(messages, truncation=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "\n",
    "\n",
    "# inference\n",
    "\n",
    "outputs = model.generate(\n",
    "\n",
    "        input_ids=input_ids,\n",
    "\n",
    "        max_new_tokens=500,\n",
    "\n",
    "        do_sample=True,\n",
    "\n",
    "        temperature=0.01,\n",
    "\n",
    "        top_k=50,\n",
    "\n",
    "        top_p=0.95\n",
    "\n",
    ")\n",
    "\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKKL0HhivQdJ"
   },
   "source": [
    "# Reference:\n",
    "\n",
    "1. https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Mistral/Supervised_fine_tuning_(SFT)_of_an_LLM_using_Hugging_Face_tooling.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T15:05:35.700918Z",
     "iopub.status.busy": "2024-11-08T15:05:35.700038Z",
     "iopub.status.idle": "2024-11-08T15:05:35.707926Z",
     "shell.execute_reply": "2024-11-08T15:05:35.706957Z",
     "shell.execute_reply.started": "2024-11-08T15:05:35.700879Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare pipeline: 0.001 sec.\n"
     ]
    }
   ],
   "source": [
    "# from time import time\n",
    "# import transformers\n",
    "# import torch\n",
    "\n",
    "# time_start = time()\n",
    "# query_pipeline = transformers.pipeline(\n",
    "#         \"text-generation\",\n",
    "#         model=model,\n",
    "#         tokenizer=tokenizer,\n",
    "#         torch_dtype=torch.float16,\n",
    "#         max_new_tokens=500,\n",
    "#         temperature=0.1,\n",
    "#         #max_length=1024,\n",
    "#         device_map=\"auto\",)\n",
    "# time_end = time()\n",
    "# print(f\"Prepare pipeline: {round(time_end-time_start, 3)} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T15:06:23.180275Z",
     "iopub.status.busy": "2024-11-08T15:06:23.179629Z",
     "iopub.status.idle": "2024-11-08T15:09:16.188409Z",
     "shell.execute_reply": "2024-11-08T15:09:16.187075Z",
     "shell.execute_reply.started": "2024-11-08T15:06:23.180225Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "# !pip install -q nltk==3.9.1 --force-reinstall --no-cache-dir\n",
    "# !pip install pandas langchain langchain_experimental langchain-groq\n",
    "# !pip install langchain-chroma langchain-text-splitters langchain-huggingface \n",
    "# !pip install \"unstructured[all-docs]\"\n",
    "# !pip install -q unstructured\n",
    "# !apt-get install -y -q poppler-utils\n",
    "# !pip install unstructured-client watermark\n",
    "# !pip install reportlab\n",
    "# !pip install PyPDF2\n",
    "# !pip install pymupdf pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T15:09:16.191189Z",
     "iopub.status.busy": "2024-11-08T15:09:16.190795Z",
     "iopub.status.idle": "2024-11-08T15:09:33.425182Z",
     "shell.execute_reply": "2024-11-08T15:09:33.424324Z",
     "shell.execute_reply.started": "2024-11-08T15:09:16.191147Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: unstructured\n",
      "Version: 0.16.5\n",
      "Summary: A library that prepares raw documents for downstream ML tasks.\n",
      "Home-page: https://github.com/Unstructured-IO/unstructured\n",
      "Author: Unstructured Technologies\n",
      "Author-email: devops@unstructuredai.io\n",
      "License: Apache-2.0\n",
      "Location: /opt/conda/lib/python3.10/site-packages\n",
      "Requires: backoff, beautifulsoup4, chardet, dataclasses-json, emoji, filetype, html5lib, langdetect, lxml, nltk, numpy, psutil, python-iso639, python-magic, python-oxmsg, rapidfuzz, requests, tqdm, typing-extensions, unstructured-client, wrapt\n",
      "Required-by: \n",
      "3.9.1\n",
      "2.9.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "# # Warning control\n",
    "# !pip show unstructured\n",
    "\n",
    "# import os\n",
    "# import base64\n",
    "# import nltk\n",
    "# print(nltk.__version__)\n",
    "# import pydantic\n",
    "# print(pydantic.__version__)\n",
    "\n",
    "# nltk.download('punkt_tab')\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "# from IPython.display import JSON\n",
    "# import json\n",
    "# from unstructured.partition.html import partition_html\n",
    "# from unstructured.partition.pdf import partition_pdf\n",
    "# from unstructured.staging.base import dict_to_elements, elements_to_json\n",
    "# import unstructured.partition\n",
    "# from unstructured.partition.pdf import partition_pdf\n",
    "# from io import StringIO \n",
    "# from lxml import etree\n",
    "# from langchain_groq import ChatGroq\n",
    "# from langchain_core.documents import Document\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "# from groq import Groq\n",
    "# from IPython.display import Image, display\n",
    "# import glob\n",
    "# from langchain.schema import Document\n",
    "# from reportlab.lib.pagesizes import letter\n",
    "# from reportlab.pdfgen import canvas\n",
    "# from groq import Groq\n",
    "# from langchain_community.document_loaders import UnstructuredPDFLoader, DirectoryLoader\n",
    "# from langchain_text_splitters import CharacterTextSplitter\n",
    "# from langchain_huggingface import HuggingFaceEmbeddings\n",
    "# from langchain_chroma import Chroma\n",
    "# from langchain_groq import ChatGroq\n",
    "# from langchain.chains import RetrievalQA\n",
    "# import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T15:09:33.427210Z",
     "iopub.status.busy": "2024-11-08T15:09:33.426483Z",
     "iopub.status.idle": "2024-11-08T15:09:39.379567Z",
     "shell.execute_reply": "2024-11-08T15:09:39.378664Z",
     "shell.execute_reply.started": "2024-11-08T15:09:33.427156Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# pdf_path = \"/kaggle/input/testing-manual/Transformer Testing Manual.pdf\"\n",
    "\n",
    "# # Load the PDF using UnstructuredPDFLoader\n",
    "# loader = UnstructuredPDFLoader(pdf_path)\n",
    "# documents = loader.load()\n",
    "\n",
    "# text_splitter = CharacterTextSplitter(\n",
    "#     chunk_size=2000,\n",
    "#     chunk_overlap=500\n",
    "# )\n",
    "\n",
    "# text_chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T15:16:29.275214Z",
     "iopub.status.busy": "2024-11-08T15:16:29.274298Z",
     "iopub.status.idle": "2024-11-08T15:16:31.070631Z",
     "shell.execute_reply": "2024-11-08T15:16:31.069799Z",
     "shell.execute_reply.started": "2024-11-08T15:16:29.275160Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30/872483988.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding = HuggingFaceEmbeddings()\n",
      "/tmp/ipykernel_30/872483988.py:2: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embedding = HuggingFaceEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "# persist_directory = \"doc_db\"\n",
    "# embedding = HuggingFaceEmbeddings()\n",
    "\n",
    "# vectorstore = Chroma.from_documents(\n",
    "#     documents=text_chunks,\n",
    "#     embedding=embedding,\n",
    "#     persist_directory=persist_directory\n",
    "# )\n",
    "\n",
    "# retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T15:09:46.241228Z",
     "iopub.status.busy": "2024-11-08T15:09:46.240923Z",
     "iopub.status.idle": "2024-11-08T15:09:46.286175Z",
     "shell.execute_reply": "2024-11-08T15:09:46.285244Z",
     "shell.execute_reply.started": "2024-11-08T15:09:46.241195Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# from torch import cuda, bfloat16\n",
    "# import torch\n",
    "# import transformers\n",
    "# from transformers import AutoTokenizer\n",
    "# from time import time\n",
    "# import chromadb\n",
    "# from chromadb.config import Settings\n",
    "# from langchain.llms import HuggingFacePipeline\n",
    "# from langchain.document_loaders import PyPDFLoader\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from langchain.embeddings import HuggingFaceEmbeddings\n",
    "# from langchain.chains import RetrievalQA\n",
    "# from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T15:09:46.287923Z",
     "iopub.status.busy": "2024-11-08T15:09:46.287599Z",
     "iopub.status.idle": "2024-11-08T15:09:46.495421Z",
     "shell.execute_reply": "2024-11-08T15:09:46.494515Z",
     "shell.execute_reply.started": "2024-11-08T15:09:46.287890Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30/725864647.py:1: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=query_pipeline)\n"
     ]
    }
   ],
   "source": [
    "# llm = HuggingFacePipeline(pipeline=query_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T15:14:04.572073Z",
     "iopub.status.busy": "2024-11-08T15:14:04.571660Z",
     "iopub.status.idle": "2024-11-08T15:15:06.341365Z",
     "shell.execute_reply": "2024-11-08T15:15:06.340353Z",
     "shell.execute_reply.started": "2024-11-08T15:14:04.572034Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, say that the question is outside of the given document, then answer the question from your knowledge. If the question is from the context, answer as normal. \n",
      "\n",
      "Context: Voltage ratio is proportionally equal to turns ratio.\n",
      "\n",
      "The TTR test gives the following:\n",
      "\n",
      "It checks the turns ratio and polarity of single- and three-phase power\n",
      "\n",
      "transformers, one phase at a time.\n",
      "\n",
      "It verifies nameplate ratio, polarity, and vectors.\n",
      "\n",
      "It checks polarity and the ratio (but not voltage rating) of transformers without\n",
      "\n",
      "markings. Tests consider all transformer no-load tap positions. Tests consider\n",
      "\n",
      "all load taps on load, tap changer (LTC) transformers if connected for voltage\n",
      "\n",
      "ratio control. On LTC transformers connected for phase angle control, ratio\n",
      "\n",
      "and polarity are completed only in neutral positions. If checked on load taps,\n",
      "\n",
      "measurements may be taken for reference for future comparison, but will\n",
      "\n",
      "deviate from nameplate ratings. LTC taps may be checked by using low three-\n",
      "\n",
      "phase voltage and reading volts and the phase angle for each.\n",
      "\n",
      "Find issues in power transformer windings, such as open-circuit and short-\n",
      "\n",
      "circuits of turn-to-turn sensitivity. The standard deviation as described in IEEE\n",
      "\n",
      "C57.12.00-2006, suggests that results should be within 0.5% of nameplate\n",
      "\n",
      "markings, with rated voltage applied to one winding. The TTR with accuracy\n",
      "\n",
      "of 0.1% is accepted as sufficiently accurate.\n",
      "\n",
      "The following steps are used for completing the TTR test:\n",
      "\n",
      "Transformer is isolated and tagged and leads disconnected\n",
      "\n",
      "Check transformer nameplate\n",
      "\n",
      "Check the polarities and vectors (phasors)\n",
      "\n",
      "Determine ratios for each no-load and load tap position\n",
      "\n",
      "The test connections are presented in Figure 1 a through c. In the case of an\n",
      "\n",
      "electronic TTR, a unity ratio check is also done, but null and zero checks are not\n",
      "\n",
      "mandatory.\n",
      "\n",
      "H\n",
      "\n",
      "X1\n",
      "\n",
      "1\n",
      "\n",
      "A\n",
      "\n",
      "(a)\n",
      "\n",
      "V\n",
      "\n",
      "N\n",
      "\n",
      "H2\n",
      "\n",
      "X2\n",
      "\n",
      "Transformer\n",
      "\n",
      "TTR test set\n",
      "\n",
      "X1\n",
      "\n",
      "H1\n",
      "\n",
      "A\n",
      "\n",
      "(b\n",
      "\n",
      "V\n",
      "\n",
      ")\n",
      "\n",
      "H2\n",
      "\n",
      "N\n",
      "\n",
      "X1\n",
      "\n",
      "Transformer\n",
      "\n",
      "TTR test set\n",
      "\n",
      "H2\n",
      "\n",
      "X1\n",
      "\n",
      "A\n",
      "\n",
      "(c)\n",
      "\n",
      "V\n",
      "\n",
      "N\n",
      "\n",
      "H2\n",
      "\n",
      "X2\n",
      "\n",
      "Transformer\n",
      "\n",
      "TTR test set\n",
      "\n",
      "Figure 1. TTR test set connections; (b) test connection for null verification for TTR\n",
      "\n",
      "and (c) test connections for zero verification for TTR\n",
      "\n",
      "Alternative Test for TTR\n",
      "\n",
      "electronic TTR, a unity ratio check is also done, but null and zero checks are not\n",
      "\n",
      "mandatory.\n",
      "\n",
      "H\n",
      "\n",
      "X1\n",
      "\n",
      "1\n",
      "\n",
      "A\n",
      "\n",
      "(a)\n",
      "\n",
      "V\n",
      "\n",
      "N\n",
      "\n",
      "H2\n",
      "\n",
      "X2\n",
      "\n",
      "Transformer\n",
      "\n",
      "TTR test set\n",
      "\n",
      "X1\n",
      "\n",
      "H1\n",
      "\n",
      "A\n",
      "\n",
      "(b\n",
      "\n",
      "V\n",
      "\n",
      ")\n",
      "\n",
      "H2\n",
      "\n",
      "N\n",
      "\n",
      "X1\n",
      "\n",
      "Transformer\n",
      "\n",
      "TTR test set\n",
      "\n",
      "H2\n",
      "\n",
      "X1\n",
      "\n",
      "A\n",
      "\n",
      "(c)\n",
      "\n",
      "V\n",
      "\n",
      "N\n",
      "\n",
      "H2\n",
      "\n",
      "X2\n",
      "\n",
      "Transformer\n",
      "\n",
      "TTR test set\n",
      "\n",
      "Figure 1. TTR test set connections; (b) test connection for null verification for TTR\n",
      "\n",
      "and (c) test connections for zero verification for TTR\n",
      "\n",
      "Alternative Test for TTR\n",
      "\n",
      "In case a TTR test set cannot be used, fast and rough test can be completed to\n",
      "\n",
      "verify the continuity and phase identification of transformer windings. The test\n",
      "\n",
      "consists of the following. The equipment required for this test is a 100 W lamp with\n",
      "\n",
      "socket and an extension cord for connection to a 120 V 60 Hz power supply, with\n",
      "\n",
      "which three test procedures are completed.\n",
      "\n",
      "Test 1: Connect the 120 V, 60 Hz power through the lamp to the transformer\n",
      "\n",
      "primary, terminals as presented in Figure 2 (a). Leave the transformer secondary\n",
      "\n",
      "winding open. The lamp will burn dimly.\n",
      "\n",
      "Test 2: Keep connections as presented in test 1, but now short the secondary\n",
      "\n",
      "winding. The lamp should burn with great brilliance. If the lamp still burns with\n",
      "\n",
      "somewhat less than full brilliance, check for issues in the transformer winding.\n",
      "\n",
      "Connections for this test are presented in Figure 2 (b).\n",
      "\n",
      "Test 3: This test is similar to tests 1 and 2, but as applied to a three phase\n",
      "\n",
      "transformer for phase identification and phase continuity check. Complete tests 1\n",
      "\n",
      "and 2 for each winding of a three-phase transformer individually with the remaining\n",
      "\n",
      "windings kept open. The test connection arrangement is shown in Figure 2 (c).\n",
      "\n",
      "(a)\n",
      "\n",
      "120V 60Hz\n",
      "\n",
      "H2\n",
      "\n",
      "X2\n",
      "\n",
      "100W\n",
      "\n",
      "X1\n",
      "\n",
      "H1\n",
      "\n",
      "(b)\n",
      "\n",
      "120V 60Hz\n",
      "\n",
      "H2\n",
      "\n",
      "X2\n",
      "\n",
      "100W\n",
      "\n",
      "X1\n",
      "\n",
      "H1\n",
      "\n",
      "H3\n",
      "\n",
      "X3\n",
      "\n",
      "(c)\n",
      "\n",
      "120V 60Hz\n",
      "\n",
      "H2\n",
      "\n",
      "X2\n",
      "\n",
      "X1\n",
      "\n",
      "100W\n",
      "\n",
      "H1\n",
      "\n",
      "X0\n",
      "\n",
      "Figure 2. Transformer winding continuity verification\n",
      "\n",
      "TTR Capacitor\n",
      "\n",
      "The TTR test may be completed at higher voltages using a capacitor in combination\n",
      "\n",
      "and 2 for each winding of a three-phase transformer individually with the remaining\n",
      "\n",
      "windings kept open. The test connection arrangement is shown in Figure 2 (c).\n",
      "\n",
      "(a)\n",
      "\n",
      "120V 60Hz\n",
      "\n",
      "H2\n",
      "\n",
      "X2\n",
      "\n",
      "100W\n",
      "\n",
      "X1\n",
      "\n",
      "H1\n",
      "\n",
      "(b)\n",
      "\n",
      "120V 60Hz\n",
      "\n",
      "H2\n",
      "\n",
      "X2\n",
      "\n",
      "100W\n",
      "\n",
      "X1\n",
      "\n",
      "H1\n",
      "\n",
      "H3\n",
      "\n",
      "X3\n",
      "\n",
      "(c)\n",
      "\n",
      "120V 60Hz\n",
      "\n",
      "H2\n",
      "\n",
      "X2\n",
      "\n",
      "X1\n",
      "\n",
      "100W\n",
      "\n",
      "H1\n",
      "\n",
      "X0\n",
      "\n",
      "Figure 2. Transformer winding continuity verification\n",
      "\n",
      "TTR Capacitor\n",
      "\n",
      "The TTR test may be completed at higher voltages using a capacitor in combination\n",
      "\n",
      "with the PF test set. With the installation of the TTR capacitor the turns ratio of power\n",
      "\n",
      "transformers at potentials up to 10 kV can be measured. Installation of the capacitor\n",
      "\n",
      "in place of the common TTR test set, allows a greater test voltage, up to 10 kV that\n",
      "\n",
      "can be applied to the primary winding. Majority of TTR test sets are limited to less\n",
      "\n",
      "than 100 V which highly decrease the voltage on the secondary windings. The\n",
      "\n",
      "advantages of the HV TTR tests are that it can discover issues and anomalies in the\n",
      "\n",
      "electric and magnetic circuit (core and coils) of the power transformer that cannot be\n",
      "\n",
      "discovered with the low-voltage TTR tests.\n",
      "\n",
      "Polarity Test\n",
      "\n",
      "The polarity test can be completed with the TTR on power, distribution, and potential\n",
      "\n",
      "transformers. Nevertheless, for current transformers the TTR test is not used.\n",
      "\n",
      "Instead a test typically known as the kick test is used. The kick test can also be used\n",
      "\n",
      "for power distribution, and potential transformers. Nevertheless, the TTR test is\n",
      "\n",
      "preferred. The connection arrangement for a kick test for a current transformer is\n",
      "\n",
      "presented in Figure 3. The DC battery voltage is typically about 7.5 V, and the multi-\n",
      "\n",
      "meter voltage range is set for 3 V full-scale. The battery negative terminal is\n",
      "\n",
      "connected to current transformer terminal H2 and the positive side is left hanging for\n",
      "\n",
      "the time being. The multi-meter positive terminal is connected to the transformer\n",
      "\n",
      "secondary terminal X1 and negative terminal to X2. To complete the test, touch the\n",
      "\n",
      "tests are used for routine maintenance, the power transformer can be examined at\n",
      "\n",
      "rated voltage for 3 min instead of testing at 65% of factory test voltage. The AC hi-\n",
      "\n",
      "pot test values for voltages up to 69 kV are presented in Table 1.\n",
      "\n",
      "Table 1. AC dielectric test for acceptance and routine maintenance for liquid-filled power transformers\n",
      "\n",
      "Transformer winding voltage (kV) 1.20 2.40 4.80 8.70 15.00 18 25.00 34.50 46.00 69.00\n",
      "\n",
      "Factory test AC voltage (kV)\n",
      "\n",
      "10 15 19 26 34 40 50 70 95 140\n",
      "\n",
      "Acceptance field test AC voltage, 75% (kV) 7.50 11.20 14.25 19.50 25.50 30.00 37.50 52.50 71.25 105.00\n",
      "\n",
      "Maintenance periodic test, 65% (kV) 6.50 9.75 12.35 16.90 22.10 26.00 32.50 45.50 61.75 91.00\n",
      "\n",
      "TTR Test\n",
      "\n",
      "During TTR test voltage is applied to one transformer winding. Also, voltage on\n",
      "\n",
      "another winding on the same core is detected. In the case of a low voltage hand-\n",
      "\n",
      "crank powered TTR, 8 V AC is applied to the tested, low-voltage transformer winding\n",
      "\n",
      "and a reference transformer in the TTR set. The HV transformer windings and the\n",
      "\n",
      "TTR reference transformer are connected through null detecting equipment. After\n",
      "\n",
      "polarity has been made at 8 V, when the null reading is zero, the dial readings show\n",
      "\n",
      "the ratio of the tested transformer.\n",
      "\n",
      "In the case of an electronic TTR test set, a voltage (usually 80 V AC) is applied to\n",
      "\n",
      "the tested transformer HV winding. The voltage detected on the low-voltage winding\n",
      "\n",
      "is measured and the voltage ratio between high and low windings is determined.\n",
      "\n",
      "Voltage ratio is proportionally equal to turns ratio.\n",
      "\n",
      "The TTR test gives the following:\n",
      "\n",
      "It checks the turns ratio and polarity of single- and three-phase power\n",
      "\n",
      "transformers, one phase at a time.\n",
      "\n",
      "It verifies nameplate ratio, polarity, and vectors.\n",
      "\n",
      "It checks polarity and the ratio (but not voltage rating) of transformers without\n",
      "\n",
      "markings. Tests consider all transformer no-load tap positions. Tests consider\n",
      "\n",
      "all load taps on load, tap changer (LTC) transformers if connected for voltage\n",
      "Question: What can be done if TTR test set cannot be used\n",
      "Answer: If a TTR test set cannot be used, a fast and rough test can be completed to verify the continuity and phase identification of transformer windings. This test uses a 100W lamp and an extension cord connected to a 120V 60Hz power supply. \n",
      "\n",
      "Time taken: 61.7601 seconds\n"
     ]
    }
   ],
   "source": [
    "# from langchain.prompts import PromptTemplate\n",
    "# from langchain.chains import RetrievalQA\n",
    "\n",
    "# template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, say that the question is outside of the given document, then answer the question from your knowledge. If the question is from the context, answer as normal. \n",
    "\n",
    "# Context: {context}\n",
    "# Question: {question}\n",
    "# Answer:\"\"\"\n",
    "\n",
    "# QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "# # Create a RetrievalQA chain with the custom prompt\n",
    "# qa_chain_HF = RetrievalQA.from_chain_type(\n",
    "#     llm=llm,\n",
    "#     chain_type=\"stuff\",\n",
    "#     retriever=retriever,\n",
    "#     chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "#     #return_source_documents=True\n",
    "# )\n",
    "\n",
    "# question = \"What can be done if TTR test set cannot be used\"\n",
    "\n",
    "# # Start the timer\n",
    "# time_start = time()\n",
    "# # Run the QA chain to get the answer\n",
    "# result = qa_chain_HF({\"query\": question})\n",
    "# # End the timer\n",
    "# time_end = time()\n",
    "\n",
    "# # Calculate the time taken to answer the question\n",
    "# elapsed_time = time_end - time_start\n",
    "\n",
    "# # Print the result and time taken\n",
    "# print(result[\"result\"])\n",
    "# print(f\"Time taken: {elapsed_time:.4f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN9eoQp4gzuWWAOOwQyEeKI",
   "gpuType": "T4",
   "mount_file_id": "1j1pnuxxiTfvtRtNq3cpz8mHhER0T7F-f",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5785685,
     "sourceId": 9505893,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6033251,
     "sourceId": 9835776,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6084100,
     "sourceId": 9903591,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6796953,
     "sourceId": 10931165,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01091e38e381443589bd34b55c9f302c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1a80ee9fd8954ee8bac710aac905d337": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1c2c818714c94a7aae16980c8a0a37e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1ec9e18c3e7a4e2b944ae1508b4ebc42": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20ebb1fd076244a4a01799368ed95aea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ac93a658d6f740a2a14c17505b3b1d69",
       "IPY_MODEL_ddd757cdc16242b1852c42dadaadbf55",
       "IPY_MODEL_2e482407d0844657ae76ee0d335bf54d"
      ],
      "layout": "IPY_MODEL_1ec9e18c3e7a4e2b944ae1508b4ebc42"
     }
    },
    "243ee195c42c4215b7fb668dfb5a1567": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c61054da71a94bacad89e4acc1224dd2",
      "max": 263,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_01091e38e381443589bd34b55c9f302c",
      "value": 263
     }
    },
    "2e482407d0844657ae76ee0d335bf54d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f3106d961547450aa23af894e235e57c",
      "placeholder": "​",
      "style": "IPY_MODEL_a7f27888df0f406da70c0cb26701b3c3",
      "value": " 27/27 [00:00&lt;00:00, 122.50 examples/s]"
     }
    },
    "3468ea53e18f487c904db761a3151e26": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d2c9e2725c4945a5b964d9b8d320a60f",
       "IPY_MODEL_8f52957ef82f41c3b324c3f5a7a5786b",
       "IPY_MODEL_40027e9711b94dbe9cc21d3741987027"
      ],
      "layout": "IPY_MODEL_99b76ece52204ad19a3ee1aa620b2e1b"
     }
    },
    "40027e9711b94dbe9cc21d3741987027": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7284c45c19144968b66a07227f6f4009",
      "placeholder": "​",
      "style": "IPY_MODEL_c37f39d8708745678e213733a5bad77b",
      "value": " 236/236 [00:01&lt;00:00, 194.92 examples/s]"
     }
    },
    "4446b0292b814bc29d8753711f578548": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "47ad4f27a4224d37b81c99e0f13c5c3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4446b0292b814bc29d8753711f578548",
      "placeholder": "​",
      "style": "IPY_MODEL_83eff657fe884153ae4f5b6a53e17bba",
      "value": "Applying chat template (num_proc=2): 100%"
     }
    },
    "4e72a5792bc54b73a98860c4621b41df": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6d4ea37f7e4b4ae193094737f1d2de5d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7284c45c19144968b66a07227f6f4009": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "813115b671be4e3699b74dbfb49ee6cd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "83eff657fe884153ae4f5b6a53e17bba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8f52957ef82f41c3b324c3f5a7a5786b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4e72a5792bc54b73a98860c4621b41df",
      "max": 236,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a262e7766edd4b5c91d1a84561b1ce40",
      "value": 236
     }
    },
    "9140231431254912b95bb9947a13a78d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_47ad4f27a4224d37b81c99e0f13c5c3e",
       "IPY_MODEL_243ee195c42c4215b7fb668dfb5a1567",
       "IPY_MODEL_92331f58c5164dfeba3429718d06f284"
      ],
      "layout": "IPY_MODEL_6d4ea37f7e4b4ae193094737f1d2de5d"
     }
    },
    "92331f58c5164dfeba3429718d06f284": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a9ff350a5c2b44deaedcf9d6094f654b",
      "placeholder": "​",
      "style": "IPY_MODEL_e1f8785e74de4938ab6fd467110f7be4",
      "value": " 263/263 [00:00&lt;00:00, 373.32 examples/s]"
     }
    },
    "99b76ece52204ad19a3ee1aa620b2e1b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a262e7766edd4b5c91d1a84561b1ce40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a7f27888df0f406da70c0cb26701b3c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a9ff350a5c2b44deaedcf9d6094f654b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ac93a658d6f740a2a14c17505b3b1d69": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e4af32265d5146ab8399ccea8c6344f1",
      "placeholder": "​",
      "style": "IPY_MODEL_1a80ee9fd8954ee8bac710aac905d337",
      "value": "Map: 100%"
     }
    },
    "b4d845cb25ea40638c08cd183ca33f33": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c37f39d8708745678e213733a5bad77b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c61054da71a94bacad89e4acc1224dd2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d2c9e2725c4945a5b964d9b8d320a60f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b4d845cb25ea40638c08cd183ca33f33",
      "placeholder": "​",
      "style": "IPY_MODEL_1c2c818714c94a7aae16980c8a0a37e0",
      "value": "Map: 100%"
     }
    },
    "ddd757cdc16242b1852c42dadaadbf55": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_813115b671be4e3699b74dbfb49ee6cd",
      "max": 27,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f8ebc6a91fbf41dfa9fb91bddd06b885",
      "value": 27
     }
    },
    "e1f8785e74de4938ab6fd467110f7be4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e4af32265d5146ab8399ccea8c6344f1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f3106d961547450aa23af894e235e57c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f8ebc6a91fbf41dfa9fb91bddd06b885": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
